{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97352500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VoiceAuthenticator:\n",
    "    def __init__(self):\n",
    "        # Thresholds derived from the research paper's observations\n",
    "        # The paper notes AI has \"narrower intra group variability\" [cite: 130]\n",
    "        # and \"standardized volume\".\n",
    "        self.THRESHOLDS = {\n",
    "            'pitch_variability_min': 20.0,    # Humans usually have > 20Hz std dev\n",
    "            'silence_ratio_max': 0.15,        # AI often has cleaner, unnatural pacing\n",
    "            'intensity_dynamic_range': 0.06   # Minimum RMS std dev for humans\n",
    "        }\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extracts the \"Trinity\" of features defined in the paper:\n",
    "        Pitch (F0), Intensity (RMS), and Frequency.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Load Audio\n",
    "            # y = audio time series, sr = sample rate\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "            # 2. Extract PITCH (Fundamental Frequency - F0)\n",
    "            # The paper states pitch is the \"primary trait\" for distinction [cite: 18]\n",
    "            # using pYIN (Probabilistic YIN) which is robust for voice\n",
    "            f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "            \n",
    "            # Filter out NaNs (unvoiced parts)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0:\n",
    "                return None, \"Audio too silent or no voice detected\"\n",
    "\n",
    "            # 3. Extract INTENSITY (Loudness)\n",
    "            # Paper: AI voices \"produce more louder outputs due to standardized volume\" \n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            \n",
    "            # 4. Extract FREQUENCY characteristics\n",
    "            # Paper: AI male voices show \"consistent negative deltas\" (compression) [cite: 206]\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "\n",
    "            features = {\n",
    "                'pitch_std': np.std(valid_f0),       # Measure of jitter/variation\n",
    "                'pitch_range': np.ptp(valid_f0),     # Max - Min pitch\n",
    "                'intensity_mean': np.mean(rms),      # Average loudness\n",
    "                'intensity_std': np.std(rms),        # Dynamic range (Emotion)\n",
    "                'freq_skew': stats.skew(spectral_centroid) # Spectral shape\n",
    "            }\n",
    "            \n",
    "            return features, None\n",
    "\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "\n",
    "    def calculate_ai_score(self, features):\n",
    "        \"\"\"\n",
    "        Generates a probability score (0.0 - 1.0) using continuous linear mapping.\n",
    "        Instead of hard thresholds, it scales the score based on how 'perfect' or 'flat' the features are.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Helper function for linear mapping (clamped between 0 and 1)\n",
    "        # Low values = High AI Score (Inverse relationship)\n",
    "        def get_linear_score(val, min_val, max_val):\n",
    "            if val <= min_val: return 1.0  # Extremely low value -> Definitely AI\n",
    "            if val >= max_val: return 0.0  # High value -> Definitely Human\n",
    "            # Linear interpolation formula\n",
    "            return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "        # --- WEIGHTS (Importance of each feature) ---\n",
    "        # Pitch is the strongest indicator (45%), followed by Intensity (35%), then Frequency Skew (20%)\n",
    "        W_PITCH = 0.45\n",
    "        W_INTENSITY = 0.35\n",
    "        W_SKEW = 0.20\n",
    "\n",
    "        explanations = []\n",
    "\n",
    "        # 1. PITCH SCORE (Linear Scale: 10Hz to 45Hz)\n",
    "        # If pitch std is < 10Hz, it's 100% AI. If > 45Hz, it's 0% AI.\n",
    "        p_score = get_linear_score(features['pitch_std'], 10.0, 45.0)\n",
    "        \n",
    "        # 2. INTENSITY SCORE (Linear Scale: 0.02 to 0.12)\n",
    "        # If intensity std is < 0.02, it's 100% AI. If > 0.12, it's 0% AI.\n",
    "        # This catches the \"0.072\" hackathon case which falls roughly in the middle (~48% AI score).\n",
    "        i_score = get_linear_score(features['intensity_std'], 0.02, 0.12)\n",
    "\n",
    "        # 3. SKEW SCORE (Linear Scale: 0.1 to 1.5)\n",
    "        # Closer to 0 means perfectly balanced (AI). \n",
    "        # We take absolute value because skew can be negative.\n",
    "        s_score = get_linear_score(abs(features['freq_skew']), 0.1, 1.5)\n",
    "\n",
    "        # --- FINAL CALCULATION ---\n",
    "        final_score = (p_score * W_PITCH) + (i_score * W_INTENSITY) + (s_score * W_SKEW)\n",
    "\n",
    "        # --- DYNAMIC EXPLANATIONS ---\n",
    "        # Only add explanations for features that contributed significantly to the AI score\n",
    "        if p_score > 0.5:\n",
    "            explanations.append(f\"Pitch is strictly constrained ({features['pitch_std']:.2f}Hz)\")\n",
    "        else:\n",
    "            explanations.append(f\"Pitch shows human variability ({features['pitch_std']:.2f}Hz)\")\n",
    "\n",
    "        if i_score > 0.5:\n",
    "            explanations.append(f\"Intensity is standardized (std: {features['intensity_std']:.3f})\")\n",
    "        \n",
    "        if s_score > 0.6: # Skew is a weaker signal, so we only mention it if it's very strong\n",
    "            explanations.append(\"Frequency distribution is statistically smoothed\")\n",
    "\n",
    "        return round(final_score, 3), \"; \".join(explanations)\n",
    "\n",
    "    def analyze(self, file_path, language=\"Unknown\"):\n",
    "        features, error = self.extract_features(file_path)\n",
    "        \n",
    "        if error:\n",
    "            return {\"status\": \"error\", \"message\": error}\n",
    "\n",
    "        ai_probability, explanation_text = self.calculate_ai_score(features)\n",
    "\n",
    "        # Decision Threshold (0.5 is neutral, >0.6 is confident AI)\n",
    "        classification = \"AI_GENERATED\" if ai_probability > 0.6 else \"HUMAN\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"language\": language,\n",
    "            \"classification\": classification,\n",
    "            \"confidenceScore\": round(ai_probability, 3),\n",
    "            \"explanation\": explanation_text,\n",
    "            \"debug_features\": features  # Helpful for your own tuning\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "\n",
    "class ResearchBackedAuthenticator:\n",
    "    def __init__(self):\n",
    "        # --- RESEARCH-DRIVEN THRESHOLDS ---\n",
    "        \n",
    "        # 1. PITCH (F0) VARIABILITY\n",
    "        # Source:\n",
    "        # Logic: \"Reduced F0 variation\" is the primary marker of synthesis.\n",
    "        # - < 15Hz: Highly unnatural (Robotic/Standard TTS)\n",
    "        # - > 45Hz: Highly dynamic (Natural/Expressive Human)\n",
    "        self.PITCH_MIN_HZ = 15.0  \n",
    "        self.PITCH_MAX_HZ = 45.0\n",
    "\n",
    "        # 2. INTENSITY (AMPLITUDE) DYNAMICS\n",
    "        # Source:\n",
    "        # Logic: AI models use normalization (standardized volume), reducing dynamic range.\n",
    "        # - < 0.04: Extremely consistent (Broadcast/AI)\n",
    "        # - > 0.12: Natural variation (Breaths, trailing off)\n",
    "        self.INTENSITY_MIN = 0.04\n",
    "        self.INTENSITY_MAX = 0.12\n",
    "\n",
    "        # 3. SPECTRAL SKEWNESS\n",
    "        # Source:\n",
    "        # Logic: AI audio has \"lower spectral complexity\" and is statistically smoother.\n",
    "        # - 0.0: Perfectly symmetrical (Mathematical generation)\n",
    "        # - > 1.0: Complex distribution (Organic/Physical source)\n",
    "        self.SKEW_MIN = 0.1\n",
    "        self.SKEW_MAX = 1.5\n",
    "\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\"\n",
    "        Calculates a 'Synthetic Probability' (0.0 to 1.0) for a single feature.\n",
    "        - Value <= min_val: 1.0 (Definitely AI)\n",
    "        - Value >= max_val: 0.0 (Definitely Human)\n",
    "        - In-between: Linearly interpolated\n",
    "        \"\"\"\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Extract Pitch (F0) using pYIN (Robust against noise)\n",
    "            f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return None, \"No voice detected\"\n",
    "\n",
    "            # Extract Intensity (RMS)\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            \n",
    "            # Extract Frequency Skewness\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            return {\n",
    "                'pitch_std': np.std(valid_f0),\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(spectral_centroid)\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "\n",
    "    def calculate_score(self, features):\n",
    "        \"\"\"\n",
    "        Computes the final 'AI Confidence Score' using weighted research parameters.\n",
    "        \"\"\"\n",
    "        # Weights based on feature reliability\n",
    "        # Pitch is the strongest biological marker.\n",
    "        W_PITCH = 0.45      \n",
    "        W_INTENSITY = 0.35  \n",
    "        W_SKEW = 0.20       \n",
    "\n",
    "        # Calculate individual feature scores\n",
    "        p_score = self.get_linear_score(features['pitch_std'], self.PITCH_MIN_HZ, self.PITCH_MAX_HZ)\n",
    "        i_score = self.get_linear_score(features['intensity_std'], self.INTENSITY_MIN, self.INTENSITY_MAX)\n",
    "        s_score = self.get_linear_score(abs(features['freq_skew']), self.SKEW_MIN, self.SKEW_MAX)\n",
    "\n",
    "        # Weighted Sum\n",
    "        final_score = (p_score * W_PITCH) + (i_score * W_INTENSITY) + (s_score * W_SKEW)\n",
    "        \n",
    "        # Generate Explanations\n",
    "        reasons = []\n",
    "        if p_score > 0.6:\n",
    "            reasons.append(f\"Low pitch variation ({features['pitch_std']:.1f}Hz) indicates synthetic constraint\")\n",
    "        elif p_score < 0.4:\n",
    "            reasons.append(f\"High pitch dynamics ({features['pitch_std']:.1f}Hz) typical of human physiology\")\n",
    "            \n",
    "        if i_score > 0.6:\n",
    "            reasons.append(\"Unnaturally consistent volume (Standardized Intensity)\")\n",
    "            \n",
    "        if s_score > 0.7:\n",
    "             reasons.append(\"Frequency distribution is statistically too smooth\")\n",
    "\n",
    "        return round(final_score, 3), \"; \".join(reasons)\n",
    "\n",
    "    def analyze(self, file_path):\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err: return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        score, explanation = self.calculate_score(feats)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if score > 0.5 else \"HUMAN\",\n",
    "            \"confidenceScore\": score,\n",
    "            \"explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Create the analyzer\n",
    "detector = ResearchBackedAuthenticator()\n",
    "\n",
    "# Replace with your actual MP3 file path\n",
    "# Note: In a real API, you would first decode the Base64 string to a temp file\n",
    "sample_file = r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\narration_20251210_232337.mp3\" \n",
    "sample_file = r\"narration_20251210_232729.mp3\"\n",
    "sample_file=r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\sample voice 1.mp3\" \n",
    "sample_file=r\"voice_preview_faiq - standard, clear and neutral.mp3\"\n",
    "sample_file=r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "\n",
    "# Run analysis\n",
    "result = detector.analyze(sample_file)\n",
    "\n",
    "# Print nicely\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "\n",
    "class GenderAdaptiveAuthenticator:\n",
    "    def __init__(self):\n",
    "        # --- RESEARCH-DRIVEN THRESHOLDS ---\n",
    "        \n",
    "        # GENDER SPLIT\n",
    "        # Source:\n",
    "        # Male F0 Range: 85-180 Hz | Female F0 Range: 165-255 Hz\n",
    "        self.GENDER_FREQ_SPLIT = 165.0 \n",
    "\n",
    "        # 1. PITCH VARIABILITY (Standard Deviation)\n",
    "        # Source: - Female speech has naturally higher variability (SD ~28Hz) than Male (~21Hz)\n",
    "        \n",
    "        # MALE THRESHOLDS\n",
    "        self.MALE_AI_MAX_STD = 15.0    # Below this is definitely AI (Robotic)\n",
    "        self.MALE_HUMAN_MIN_STD = 35.0 # Above this is definitely Human\n",
    "        \n",
    "        # FEMALE THRESHOLDS (Shifted UP)\n",
    "        # Your error happened because 36.8Hz is \"Human\" for a male, but \"Robotic\" for a female.\n",
    "        self.FEMALE_AI_MAX_STD = 40.0   # Expanded range for Female AI\n",
    "        self.FEMALE_HUMAN_MIN_STD = 60.0 # Females need much more variance to be \"Human\"\n",
    "\n",
    "        # 2. INTENSITY & SKEW (Gender neutral)\n",
    "        self.INTENSITY_MIN = 0.04\n",
    "        self.INTENSITY_MAX = 0.12\n",
    "        self.SKEW_MIN = 0.1\n",
    "        self.SKEW_MAX = 1.5\n",
    "\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\"\n",
    "        Calculates Synthetic Probability (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Extract Pitch (F0)\n",
    "            f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return None, \"No voice detected\"\n",
    "\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            return {\n",
    "                'mean_f0': np.mean(valid_f0), # Used for Gender Detection\n",
    "                'pitch_std': np.std(valid_f0),\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(spectral_centroid)\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "\n",
    "    def calculate_score(self, features):\n",
    "        explanations = []\n",
    "        \n",
    "        # --- STEP 1: DETECT GENDER ---\n",
    "        if features['mean_f0'] > self.GENDER_FREQ_SPLIT:\n",
    "            gender = \"Female\"\n",
    "            # Use stricter Female Thresholds\n",
    "            p_min = self.FEMALE_AI_MAX_STD\n",
    "            p_max = self.FEMALE_HUMAN_MIN_STD\n",
    "            explanations.append(f\"Voice detected as Female (Mean F0: {features['mean_f0']:.0f}Hz)\")\n",
    "        else:\n",
    "            gender = \"Male\"\n",
    "            # Use Standard Male Thresholds\n",
    "            p_min = self.MALE_AI_MAX_STD\n",
    "            p_max = self.MALE_HUMAN_MIN_STD\n",
    "            explanations.append(f\"Voice detected as Male (Mean F0: {features['mean_f0']:.0f}Hz)\")\n",
    "\n",
    "        # --- STEP 2: CALCULATE SCORES ---\n",
    "        \n",
    "        # Pitch Score (Dynamic based on Gender)\n",
    "        p_score = self.get_linear_score(features['pitch_std'], p_min, p_max)\n",
    "        \n",
    "        # Intensity & Skew (Global)\n",
    "        i_score = self.get_linear_score(features['intensity_std'], self.INTENSITY_MIN, self.INTENSITY_MAX)\n",
    "        s_score = self.get_linear_score(abs(features['freq_skew']), self.SKEW_MIN, self.SKEW_MAX)\n",
    "\n",
    "        # Weighted Sum\n",
    "        W_PITCH = 0.45\n",
    "        W_INTENSITY = 0.35\n",
    "        W_SKEW = 0.20\n",
    "        \n",
    "        final_score = (p_score * W_PITCH) + (i_score * W_INTENSITY) + (s_score * W_SKEW)\n",
    "\n",
    "        # --- DYNAMIC EXPLANATION ---\n",
    "        if p_score > 0.5:\n",
    "            explanations.append(f\"Pitch variance ({features['pitch_std']:.1f}Hz) is too low for a natural {gender} voice\")\n",
    "        else:\n",
    "            explanations.append(f\"Pitch variance ({features['pitch_std']:.1f}Hz) aligns with natural {gender} physiology\")\n",
    "\n",
    "        if i_score > 0.6:\n",
    "            explanations.append(\"Intensity is standardized (AI artifact)\")\n",
    "\n",
    "        return round(final_score, 3), \"; \".join(explanations)\n",
    "\n",
    "    def analyze(self, file_path):\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err: return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        score, explanation = self.calculate_score(feats)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if score > 0.55 else \"HUMAN\",\n",
    "            \"confidenceScore\": score,\n",
    "            \"explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa70887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Create the analyzer\n",
    "detector = GenderAdaptiveAuthenticator()\n",
    "\n",
    "# Replace with your actual MP3 file path\n",
    "# Note: In a real API, you would first decode the Base64 string to a temp file\n",
    "sample_file = r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\narration_20251210_232337.mp3\" \n",
    "# sample_file = r\"narration_20251210_232729.mp3\"\n",
    "sample_file=r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\sample voice 1.mp3\" \n",
    "# sample_file=r\"voice_preview_faiq - standard, clear and neutral.mp3\"\n",
    "# sample_file=r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "sample_file=r\"voice_preview_kanika - soft, smooth and muffled.mp3\"\n",
    "\n",
    "# Run analysis\n",
    "result = detector.analyze(sample_file)\n",
    "\n",
    "# Print nicely\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "\n",
    "class RobustVoiceAuthenticator:\n",
    "    def __init__(self):\n",
    "        # --- RESEARCH-DRIVEN THRESHOLDS ---\n",
    "        \n",
    "        # 1. COEFFICIENT OF VARIATION (CV) - The \"Gender-Agnostic\" Metric\n",
    "        # CV = Standard Deviation / Mean Pitch\n",
    "        # Logic: Normalizes variation. \n",
    "        # - AI (Smoothed): CV < 0.18 (Variation is small relative to pitch)\n",
    "        # - Human (Dynamic): CV > 0.25 (Variation is distinct relative to pitch)\n",
    "        # This works for both 110Hz (Male) and 220Hz (Female) equally.\n",
    "        self.CV_AI_THRESHOLD = 0.18\n",
    "        self.CV_HUMAN_THRESHOLD = 0.25\n",
    "\n",
    "        # 2. INTENSITY (AMPLITUDE) - The \"Lie Detector\" \n",
    "        # Paper: AI Intensity has ~0 correlation with humans. It is the strongest tell.\n",
    "        self.INTENSITY_MIN_STD = 0.04  # Very Flat (AI)\n",
    "        self.INTENSITY_MAX_STD = 0.11  # Very Dynamic (Human)\n",
    "\n",
    "        # 3. SPECTRAL SKEW\n",
    "        self.SKEW_AI_THRESHOLD = 0.5  # Too symmetrical\n",
    "\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\" Returns 1.0 for AI-like values (<= min), 0.0 for Human-like (>= max) \"\"\"\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            # Load audio (mono)\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Use 'sr' in pyin to help it guess range better, but keep bounds wide\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return None, \"No voice detected\"\n",
    "\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            return {\n",
    "                'pitch_cv': std_pitch / mean_pitch,  # The new metric\n",
    "                'pitch_std': std_pitch,\n",
    "                'mean_pitch': mean_pitch,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "\n",
    "    def calculate_score(self, features):\n",
    "        explanations = []\n",
    "        \n",
    "        # --- 1. INTENSITY SCORE (Primary Discriminator) ---\n",
    "        # Weight increased because Paper  says Pitch can be faked, Intensity cannot.\n",
    "        i_score = self.get_linear_score(features['intensity_std'], self.INTENSITY_MIN_STD, self.INTENSITY_MAX_STD)\n",
    "        \n",
    "        # --- 2. PITCH CV SCORE (Normalized Variability) ---\n",
    "        # Uses relative variation (CV) to solve the 110Hz vs 220Hz issue.\n",
    "        p_score = self.get_linear_score(features['pitch_cv'], self.CV_AI_THRESHOLD, self.CV_HUMAN_THRESHOLD)\n",
    "        \n",
    "        # --- 3. SKEW SCORE ---\n",
    "        s_score = self.get_linear_score(abs(features['freq_skew']), 0.1, 1.0)\n",
    "\n",
    "        # --- SCORING LOGIC ---\n",
    "        # Standard Weighted Average\n",
    "        # We give Intensity higher weight now.\n",
    "        W_INTENSITY = 0.50  #  \"Intensity... weak or even negative correlation\"\n",
    "        W_PITCH = 0.30      #  \"High positive correlation\" (Can be faked)\n",
    "        W_SKEW = 0.20\n",
    "        \n",
    "        base_score = (i_score * W_INTENSITY) + (p_score * W_PITCH) + (s_score * W_SKEW)\n",
    "\n",
    "        # --- THE \"VETO\" RULE ---\n",
    "        # If Intensity is STRONGLY AI (Standardized), we force the score up.\n",
    "        # Even if pitch variation is perfect (because of ElevenLabs), standard volume reveals it.\n",
    "        if i_score > 0.8: \n",
    "            final_score = max(base_score, 0.75) # Force classification to AI\n",
    "            explanations.append(f\"⚠️ Intensity is highly standardized (Score {i_score:.2f}), overriding pitch metrics.\")\n",
    "        else:\n",
    "            final_score = base_score\n",
    "\n",
    "        # Explanations\n",
    "        if i_score > 0.6:\n",
    "            explanations.append(f\"Volume is unnaturally consistent (std: {features['intensity_std']:.3f})\")\n",
    "        if p_score > 0.6:\n",
    "            explanations.append(f\"Pitch modulation is constrained (CV: {features['pitch_cv']:.2f})\")\n",
    "        elif p_score < 0.4:\n",
    "            explanations.append(f\"Pitch is dynamic (CV: {features['pitch_cv']:.2f})\")\n",
    "\n",
    "        return round(final_score, 3), \"; \".join(explanations)\n",
    "\n",
    "    def analyze(self, file_path):\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err: return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        score, explanation = self.calculate_score(feats)\n",
    "        \n",
    "        # Threshold 0.55 allows a small margin of error\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if score > 0.55 else \"HUMAN\",\n",
    "            \"confidenceScore\": score,\n",
    "            \"explanation\": explanation,\n",
    "            \"debug\": feats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Create the analyzer\n",
    "detector = RobustVoiceAuthenticator()\n",
    "\n",
    "# Replace with your actual MP3 file path\n",
    "# Note: In a real API, you would first decode the Base64 string to a temp file\n",
    "sample_file = r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\narration_20251210_232337.mp3\" \n",
    "sample_file = r\"narration_20251210_232729.mp3\"\n",
    "sample_file=r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\sample voice 1.mp3\" \n",
    "# sample_file=r\"voice_preview_faiq - standard, clear and neutral.mp3\"\n",
    "# sample_file=r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "# sample_file=r\"voice_preview_kanika - soft, smooth and muffled.mp3\"\n",
    "\n",
    "# Run analysis\n",
    "result = detector.analyze(sample_file)\n",
    "\n",
    "# Print nicely\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "\n",
    "class FinalVoiceAuthenticator:\n",
    "    def __init__(self):\n",
    "        # --- TUNED PARAMETERS ---\n",
    "        \n",
    "        # 1. PITCH CV (Coefficient of Variation)\n",
    "        # We raised the 'Human' bar slightly. \n",
    "        # Before: 0.25 was guaranteed Human. Now: 0.32 is guaranteed Human.\n",
    "        # This makes 0.19 (your sample) look more \"AI-like\".\n",
    "        self.CV_AI_THRESHOLD = 0.15      # Strictly Robotic\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32   # Natural Expressiveness\n",
    "\n",
    "        # 2. INTENSITY (Loudness)\n",
    "        # We shifted the window to catch the \"0.072\" case.\n",
    "        # Anything below 0.05 is definite AI.\n",
    "        # Anything above 0.15 is definite Human.\n",
    "        self.INTENSITY_MIN_STD = 0.03\n",
    "        self.INTENSITY_MAX_STD = 0.15 \n",
    "\n",
    "        # 3. SPECTRAL SKEW\n",
    "        self.SKEW_AI_THRESHOLD = 0.5\n",
    "\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\" Returns 1.0 for AI (<= min), 0.0 for Human (>= max) \"\"\"\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Robust Pitch Tracking\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return None, \"No voice detected\"\n",
    "\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            return {\n",
    "                'pitch_cv': std_pitch / mean_pitch,\n",
    "                'pitch_std': std_pitch,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "\n",
    "    def calculate_score(self, features):\n",
    "        explanations = []\n",
    "        \n",
    "        # --- CALCULATE RAW SCORES ---\n",
    "        i_score = self.get_linear_score(features['intensity_std'], self.INTENSITY_MIN_STD, self.INTENSITY_MAX_STD)\n",
    "        p_score = self.get_linear_score(features['pitch_cv'], self.CV_AI_THRESHOLD, self.CV_HUMAN_THRESHOLD)\n",
    "        s_score = self.get_linear_score(abs(features['freq_skew']), 0.1, 1.0)\n",
    "\n",
    "        # --- WEIGHTS ---\n",
    "        # Pitch is slightly more reliable for high-quality clones, \n",
    "        # Intensity is the fallback for \"perfect\" clones.\n",
    "        W_INTENSITY = 0.40\n",
    "        W_PITCH = 0.40\n",
    "        W_SKEW = 0.20\n",
    "        \n",
    "        base_score = (i_score * W_INTENSITY) + (p_score * W_PITCH) + (s_score * W_SKEW)\n",
    "\n",
    "        # --- THE SYNERGY BONUS (The Fix for 0.499) ---\n",
    "        # If BOTH Pitch and Intensity are \"Suspicious\" (> 0.4),\n",
    "        # it is highly unlikely to be a Human. Humans usually trade off (Monotone but loud, or Quiet but expressive).\n",
    "        # We add a 0.15 boost if both metrics are flagging.\n",
    "        if i_score > 0.4 and p_score > 0.4:\n",
    "            final_score = min(base_score + 0.15, 1.0)\n",
    "            explanations.append(\"Combined lack of Pitch and Intensity dynamics suggests synthesis\")\n",
    "        else:\n",
    "            final_score = base_score\n",
    "\n",
    "        # --- EXPLANATIONS ---\n",
    "        if final_score > 0.5:\n",
    "            if i_score > 0.5: explanations.append(f\"Intensity is standardized (std: {features['intensity_std']:.3f})\")\n",
    "            if p_score > 0.5: explanations.append(f\"Pitch is constrained (CV: {features['pitch_cv']:.2f})\")\n",
    "        else:\n",
    "             explanations.append(\"Voice exhibits natural variability in pitch and loudness\")\n",
    "\n",
    "        return round(final_score, 3), \"; \".join(explanations)\n",
    "\n",
    "    def analyze(self, file_path):\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err: return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        score, explanation = self.calculate_score(feats)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if score > 0.55 else \"HUMAN\",\n",
    "            \"confidenceScore\": score,\n",
    "            \"explanation\": explanation,\n",
    "            \"debug\": feats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Create the analyzer\n",
    "detector = FinalVoiceAuthenticator()\n",
    "\n",
    "# Replace with your actual MP3 file path\n",
    "# Note: In a real API, you would first decode the Base64 string to a temp file\n",
    "sample_file = r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\narration_20251210_232337.mp3\" \n",
    "# sample_file = r\"narration_20251210_232729.mp3\"\n",
    "sample_file=r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\sample voice 1.mp3\" \n",
    "# sample_file=r\"voice_preview_faiq - standard, clear and neutral.mp3\"\n",
    "# sample_file=r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "# sample_file=r\"voice_preview_kanika - soft, smooth and muffled.mp3\"\n",
    "sample_file=r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\"\n",
    "sample_file=r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\"\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "result = detector.analyze(sample_file)\n",
    "\n",
    "# Print nicely\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bedcc4",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f982fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "CHUNK_DURATION = 10.0  # Seconds per sample\n",
    "# We need to skip the first/last few seconds as they might be silence\n",
    "OFFSET = 10.0         \n",
    "\n",
    "# --- UPDATED FEATURE EXTRACTOR ---\n",
    "def extract_features_from_chunk(y, sr):\n",
    "    try:\n",
    "        # 1. Standard Features (Keep these for Explanation)\n",
    "        f0 = librosa.yin(y, fmin=50, fmax=400, sr=sr)\n",
    "        valid_f0 = f0[~np.isnan(f0)]\n",
    "        if len(valid_f0) == 0: return None\n",
    "        \n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        \n",
    "        # 2. MFCCs (The Accuracy Boosters)\n",
    "        # We take the mean of 13 MFCC coefficients\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_means = np.mean(mfccs, axis=1) # Results in an array of 13 numbers\n",
    "        \n",
    "        # Build the dictionary\n",
    "        features = {\n",
    "            'pitch_cv': np.std(valid_f0) / np.mean(valid_f0),\n",
    "            'intensity_std': np.std(rms),\n",
    "            'freq_skew': stats.skew(centroid),\n",
    "        }\n",
    "        \n",
    "        # Add MFCCs as separate columns (mfcc_0 to mfcc_12)\n",
    "        for i, val in enumerate(mfcc_means):\n",
    "            features[f'mfcc_{i}'] = val\n",
    "            \n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_dataset(base_folder):\n",
    "    data_records = []\n",
    "    \n",
    "    # Define classes based on folder names\n",
    "    classes = {'ai_voices': 1, 'human_voices': 0} # 1 = AI, 0 = Human\n",
    "    \n",
    "    for label_name, label_code in classes.items():\n",
    "        folder_path = os.path.join(base_folder, label_name)\n",
    "        audio_files = glob(folder_path + \"/*.wav\") # Finds all .wav files\n",
    "        \n",
    "        print(f\"Processing {label_name}: Found {len(audio_files)} files...\")\n",
    "        \n",
    "        for file in audio_files:\n",
    "            try:\n",
    "                # Load the FULL file\n",
    "                y_full, sr = librosa.load(file, sr=None)\n",
    "                total_duration = librosa.get_duration(y=y_full, sr=sr)\n",
    "                \n",
    "                # Slicing Logic\n",
    "                # We start at OFFSET and take 5s chunks until the end\n",
    "                num_chunks = int((total_duration - (OFFSET*2)) // CHUNK_DURATION)\n",
    "                \n",
    "                # Limit chunks per file to avoid dataset imbalance \n",
    "                # (e.g., max 50 chunks per file)\n",
    "                num_chunks = min(num_chunks, 50) \n",
    "                \n",
    "                for i in range(num_chunks):\n",
    "                    start_sample = int((OFFSET + i * CHUNK_DURATION) * sr)\n",
    "                    end_sample = int(start_sample + (CHUNK_DURATION * sr))\n",
    "                    \n",
    "                    y_chunk = y_full[start_sample:end_sample]\n",
    "                    \n",
    "                    # Extract features\n",
    "                    feats = extract_features_from_chunk(y_chunk, sr)\n",
    "                    if feats:\n",
    "                        feats['label'] = label_code # Add the answer key\n",
    "                        feats['source_file'] = os.path.basename(file)\n",
    "                        data_records.append(feats)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                \n",
    "    return pd.DataFrame(data_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTE ---\n",
    "# 1. Update this path to where you extracted your dataset\n",
    "dataset_path = \"AI_voice_dataset/training\" \n",
    "\n",
    "print(\"Starting Feature Extraction... This may take a few minutes.\")\n",
    "df = process_dataset(dataset_path)\n",
    "\n",
    "# 2. Save to CSV so we don't have to wait again\n",
    "df.to_csv(\"trained_voice_features.csv\", index=False)\n",
    "print(f\"Done! Saved {len(df)} training samples to 'trained_voice_features.csv'.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib # To save the model\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(\"trained_voice_features.csv\")\n",
    "\n",
    "# Define Predictors: 3 Physics Features + 13 MFCCs\n",
    "feature_cols = ['pitch_cv', 'intensity_std', 'freq_skew'] + [f'mfcc_{i}' for i in range(13)]\n",
    "\n",
    "X = df[feature_cols] # Now X has 16 columns instead of 3\n",
    "y = df['label']\n",
    "\n",
    "# 3. Train/Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Initialize and Train Random Forest\n",
    "# n_estimators=100 means it builds 100 decision trees\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(\"\\nDetailed Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['HUMAN', 'AI']))\n",
    "\n",
    "# 6. Save the trained brain to a file\n",
    "joblib.dump(clf, \"voice_auth_model.pkl\")\n",
    "print(\"Model saved as 'voice_auth_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "class VoiceAuthModel:\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load(\"voice_auth_model.pkl\")\n",
    "\n",
    "    def analyze_with_ml(self, file_path):\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err: return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        # 1. PREPARE INPUT VECTOR (16 Features)\n",
    "        # Order: [pitch_cv, intensity_std, skew, mfcc_0 ... mfcc_12]\n",
    "        input_vector = [\n",
    "            feats['pitch_cv'], \n",
    "            feats['intensity_std'], \n",
    "            feats['freq_skew']\n",
    "        ] + feats['mfccs'].tolist() # Add the list of 13 MFCCs\n",
    "        \n",
    "        # 2. PREDICT\n",
    "        # Reshape to 2D array for sklearn: [[col1, col2...]]\n",
    "        prediction = self.model.predict([input_vector])[0] \n",
    "        probability = self.model.predict_proba([input_vector])[0][1]\n",
    "\n",
    "        # 3. GENERATE EXPLANATION (Using only the readable features)\n",
    "        # We don't explain MFCCs to humans; we explain Pitch/Intensity.\n",
    "        explanation_parts = []\n",
    "        \n",
    "        # Logic: If model says AI, find the \"AI-like\" physical traits to blame\n",
    "        if prediction == 1:\n",
    "            if feats['intensity_std'] < 0.05:\n",
    "                explanation_parts.append(f\"Standardized intensity (AI)({feats['intensity_std']:.3f})\")\n",
    "            if feats['pitch_cv'] < 0.20:\n",
    "                explanation_parts.append(f\"Robotic pitch variation (CV: {feats['pitch_cv']:.2f})\")\n",
    "            if not explanation_parts:\n",
    "                explanation_parts.append(\"Synthetic timbre artifacts detected in MFCC analysis\")\n",
    "        \n",
    "        # Logic: If model says Human, highlight the dynamic traits\n",
    "        else:\n",
    "            if feats['intensity_std'] > 0.10:\n",
    "                explanation_parts.append(\"Natural dynamic intensity range\")\n",
    "            if feats['pitch_cv'] > 0.25:\n",
    "                explanation_parts.append(\"Human-like pitch modulation\")\n",
    "            if not explanation_parts:\n",
    "                explanation_parts.append(\"Acoustic profile matches natural human speech\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if prediction == 1 else \"HUMAN\",\n",
    "            \"confidenceScore\": round(probability, 3),\n",
    "            \"explanation\": \"; \".join(explanation_parts)\n",
    "        }\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Physics Features\n",
    "            f0 = librosa.yin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            if len(valid_f0) == 0: return None, \"No voice\"\n",
    "            \n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            # MFCC Features (Mean of each band)\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "            mfcc_means = np.mean(mfccs, axis=1)\n",
    "\n",
    "            return {\n",
    "                'pitch_cv': np.std(valid_f0) / np.mean(valid_f0),\n",
    "                'pitch_std': np.std(valid_f0),\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid),\n",
    "                'mfccs': mfcc_means # Store raw MFCCs for the model\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import joblib  # Required for loading the model\n",
    "\n",
    "class VoiceAuthModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load the trained Random Forest model\n",
    "        self.model = joblib.load(\"voice_auth_model.pkl\")\n",
    "\n",
    "    def analyze_with_ml(self, file_path):\n",
    "        # 1. Extract features\n",
    "        feats, err = self.extract_features(file_path)\n",
    "        if err:\n",
    "            return {\"status\": \"error\", \"message\": err}\n",
    "        \n",
    "        # 2. Prepare data (Ensure exact column order as training!)\n",
    "        input_data = [[feats['pitch_cv'], feats['intensity_std'], feats['freq_skew']]]\n",
    "        \n",
    "        # 3. Predict using ML\n",
    "        prediction = self.model.predict(input_data)[0]  # 0 (Human) or 1 (AI)\n",
    "        probability = self.model.predict_proba(input_data)[0][1]  # Probability of class 1 (AI)\n",
    "        \n",
    "        # 4. Generate Dynamic Explanation (The \"Why\")\n",
    "        # Even though the ML decides the score, we explain it using the feature values.\n",
    "        explanation_parts = []\n",
    "        \n",
    "        # Intensity Analysis\n",
    "        if feats['intensity_std'] < 0.05:\n",
    "            explanation_parts.append(f\"Standardized intensity ({feats['intensity_std']:.3f}) typical of AI\")\n",
    "        elif feats['intensity_std'] > 0.10:\n",
    "             explanation_parts.append(\"Dynamic intensity indicates human emotion\")\n",
    "             \n",
    "        # Pitch Analysis\n",
    "        if feats['pitch_cv'] < 0.18:\n",
    "            explanation_parts.append(f\"Pitch variation is robotic (CV: {feats['pitch_cv']:.2f})\")\n",
    "        elif feats['pitch_cv'] > 0.30:\n",
    "            explanation_parts.append(\"Natural pitch modulation detected\")\n",
    "            \n",
    "        # Fallback if no specific feature triggered\n",
    "        if not explanation_parts:\n",
    "            explanation_parts.append(\"Acoustic profile matches learned AI patterns\" if prediction == 1 else \"Acoustic profile consistent with human speech\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": \"AI_GENERATED\" if prediction == 1 else \"HUMAN\",\n",
    "            \"confidenceScore\": round(probability, 3),\n",
    "            \"explanation\": \"; \".join(explanation_parts)\n",
    "        }\n",
    "\n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            # Load Audio\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Extract Pitch (Robust pYIN is better for single-file inference than yin)\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return None, \"No voice detected\"\n",
    "\n",
    "            # Extract other features\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            return {\n",
    "                'pitch_cv': std_pitch / mean_pitch,\n",
    "                'pitch_std': std_pitch,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Create the analyzer\n",
    "detector = VoiceAuthModel()\n",
    "\n",
    "# Replace with your actual MP3 file path\n",
    "# Note: In a real API, you would first decode the Base64 string to a temp file\n",
    "sample_file = r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\narration_20251210_232337.mp3\" \n",
    "# sample_file = r\"narration_20251210_232729.mp3\"\n",
    "# sample_file=r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\sample voice 1.mp3\" \n",
    "# sample_file=r\"voice_preview_faiq - standard, clear and neutral.mp3\"\n",
    "# sample_file=r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "# sample_file=r\"voice_preview_kanika - soft, smooth and muffled.mp3\"\n",
    "\n",
    "# Run analysis\n",
    "result = detector.analyze_with_ml(sample_file)\n",
    "\n",
    "# Print nicely\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d05330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TEST_FOLDER = \"AI_voice_dataset/testing\"  # Folder containing 'ai_voices' and 'human_voices'\n",
    "MODEL_FILE = \"voice_auth_model.pkl\"\n",
    "CHUNK_DURATION = 5.0\n",
    "OFFSET = 2.0  # Skip start to avoid silence\n",
    "\n",
    "# --- 1. FEATURE EXTRACTION (MUST MATCH TRAINING EXACTLY) ---\n",
    "def extract_features_from_chunk(y, sr):\n",
    "    try:\n",
    "        # Physics Features\n",
    "        f0 = librosa.yin(y, fmin=50, fmax=400, sr=sr)\n",
    "        valid_f0 = f0[~np.isnan(f0)]\n",
    "        if len(valid_f0) == 0: return None\n",
    "        \n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        \n",
    "        # MFCC Features (13 coeffs)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_means = np.mean(mfccs, axis=1) # 13 values\n",
    "        \n",
    "        # Build Vector: [pitch_cv, intensity_std, freq_skew, mfcc_0 ... mfcc_12]\n",
    "        # This ORDER is critical. It must match X_train columns.\n",
    "        features = [\n",
    "            np.std(valid_f0) / np.mean(valid_f0), # pitch_cv\n",
    "            np.std(rms),                          # intensity_std\n",
    "            stats.skew(centroid)                  # freq_skew\n",
    "        ] + mfcc_means.tolist()                   # Add 13 MFCCs\n",
    "        \n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 2. BATCH TESTING FUNCTION ---\n",
    "def evaluate_test_set(model_path, data_path):\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    clf = joblib.load(model_path)\n",
    "    \n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    file_names = []\n",
    "    \n",
    "    classes = {'ai_voices': 1, 'human_voices': 0}\n",
    "    \n",
    "    print(\"Processing Test Files...\")\n",
    "    \n",
    "    for label_name, label_code in classes.items():\n",
    "        folder = os.path.join(data_path, label_name)\n",
    "        files = glob(folder + \"/*.wav\")\n",
    "        \n",
    "        for file in files:\n",
    "            try:\n",
    "                # Load file\n",
    "                y_full, sr = librosa.load(file, sr=None)\n",
    "                total_duration = librosa.get_duration(y=y_full, sr=sr)\n",
    "                \n",
    "                # We test on chunks just like we trained\n",
    "                num_chunks = int((total_duration - (OFFSET*2)) // CHUNK_DURATION)\n",
    "                num_chunks = min(num_chunks, 10) # Test max 10 chunks per file to save time\n",
    "                \n",
    "                for i in range(num_chunks):\n",
    "                    start = int((OFFSET + i * CHUNK_DURATION) * sr)\n",
    "                    end = int(start + (CHUNK_DURATION * sr))\n",
    "                    y_chunk = y_full[start:end]\n",
    "                    \n",
    "                    feats = extract_features_from_chunk(y_chunk, sr)\n",
    "                    \n",
    "                    if feats:\n",
    "                        X_test.append(feats)\n",
    "                        y_test.append(label_code)\n",
    "                        file_names.append(os.path.basename(file))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Convert to Numpy for Sklearn\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    print(f\"\\n--- RESULTS ({len(y_test)} samples) ---\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"FINAL ACCURACY: {acc * 100:.2f}%\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[0, 1]))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['HUMAN', 'AI']))\n",
    "\n",
    "# --- 3. SINGLE FILE CHECKER (For your Hackathon Sample) ---\n",
    "def test_single_file(file_path):\n",
    "    clf = joblib.load(MODEL_FILE)\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Grab a 5s chunk from the middle\n",
    "    mid = len(y) // 2\n",
    "    chunk_len = int(5.0 * sr)\n",
    "    y_chunk = y[mid : mid + chunk_len]\n",
    "    \n",
    "    feats = extract_features_from_chunk(y_chunk, sr)\n",
    "    if not feats:\n",
    "        print(\"Could not extract features.\")\n",
    "        return\n",
    "\n",
    "    # Reshape for model [[col1, col2...]]\n",
    "    pred = clf.predict([feats])[0]\n",
    "    prob = clf.predict_proba([feats])[0][1]\n",
    "    \n",
    "    print(f\"\\n--- SINGLE FILE ANALYSIS: {file_path} ---\")\n",
    "    print(f\"Prediction: {'AI_GENERATED' if pred == 1 else 'HUMAN'}\")\n",
    "    print(f\"Confidence: {prob:.4f}\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run the full evaluation on the folder\n",
    "    evaluate_test_set(MODEL_FILE, TEST_FOLDER)\n",
    "    \n",
    "    # 2. (Optional) Test your specific Hackathon file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da700838",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_file(\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\")\n",
    "test_single_file(r\"D:\\hackathons\\GUVI_HCL\\AI_Voice_Detector\\voice_preview_tarini - soft, cheerful and expressive.mp3\")\n",
    "test_single_file(r\"sample voice 1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67629cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    def __init__(self, model_path=\"wav2vec2_finetuned_model\"):\n",
    "        # --- 1. SETUP WAV2VEC2 (The Deep Learning Brain) ---\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Loading Wav2Vec2 model from '{model_path}' on {self.device}...\")\n",
    "        \n",
    "        try:\n",
    "            self.dl_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "            self.dl_model.to(self.device)\n",
    "            self.dl_model.eval()\n",
    "            self.dl_ready = True\n",
    "            print(\"✅ Wav2Vec2 Model Loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Wav2Vec2 Load Failed: {e}. Running in Physics-Only mode.\")\n",
    "            self.dl_ready = False\n",
    "\n",
    "        self.id2label = {\n",
    "            0: \"diffwave\", 1: \"melgan\", 2: \"parallel_wave_gan\", \n",
    "            3: \"Real\", 4: \"wavegrad\", 5: \"wavnet\", 6: \"wavernn\"\n",
    "        }\n",
    "\n",
    "        # --- 2. SETUP PHYSICS PARAMETERS (The Logic Brain) ---\n",
    "        # (Your Tuned Parameters)\n",
    "        self.CV_AI_THRESHOLD = 0.15\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32\n",
    "        self.INTENSITY_MIN_STD = 0.03\n",
    "        self.INTENSITY_MAX_STD = 0.15 \n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: PHYSICS ENGINE (Your Code)\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        try:\n",
    "            # Load Audio (Native SR)\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Robust Pitch Tracking\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return 0.0, \"No voice detected\", {}\n",
    "\n",
    "            # Feature Extraction\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }\n",
    "\n",
    "            # Scoring Logic\n",
    "            i_score = self.get_linear_score(feats['intensity_std'], self.INTENSITY_MIN_STD, self.INTENSITY_MAX_STD)\n",
    "            p_score = self.get_linear_score(feats['pitch_cv'], self.CV_AI_THRESHOLD, self.CV_HUMAN_THRESHOLD)\n",
    "            s_score = self.get_linear_score(abs(feats['freq_skew']), 0.1, 1.0)\n",
    "\n",
    "            # Weights\n",
    "            W_INTENSITY = 0.40\n",
    "            W_PITCH = 0.40\n",
    "            W_SKEW = 0.20\n",
    "            \n",
    "            base_score = (i_score * W_INTENSITY) + (p_score * W_PITCH) + (s_score * W_SKEW)\n",
    "\n",
    "            # Synergy Bonus (The \"ElevenLabs Trap\")\n",
    "            if i_score > 0.4 and p_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: WAV2VEC2 ENGINE (Deep Learning)\n",
    "    # ==========================================================\n",
    "    # ==========================================================\n",
    "    # PART B: WAV2VEC2 ENGINE (Updated to use Librosa)\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        if not self.dl_ready: return 0.0, \"Model not loaded\"\n",
    "\n",
    "        target_sr = 16000\n",
    "        max_len = target_sr * 10 # 10 seconds\n",
    "\n",
    "        try:\n",
    "            # 1. Load with Librosa (Bypassing Torchaudio errors)\n",
    "            # Librosa loads as (n_samples,), floating point -1 to 1\n",
    "            waveform_np, sr = librosa.load(audio_path, sr=target_sr) # Force resample here\n",
    "\n",
    "            # 2. Convert to Torch Tensor\n",
    "            waveform = torch.tensor(waveform_np).unsqueeze(0) # Shape: (1, n_samples)\n",
    "\n",
    "            # 3. Pad/Truncate\n",
    "            if waveform.size(1) > max_len: \n",
    "                waveform = waveform[:, :max_len]\n",
    "            elif waveform.size(1) < max_len:\n",
    "                waveform = F.pad(waveform, (0, max_len - waveform.size(1)))\n",
    "\n",
    "            # 4. Predict\n",
    "            inputs = self.processor(waveform.squeeze().numpy(), sampling_rate=target_sr, return_tensors=\"pt\", padding=True)\n",
    "            input_values = inputs.input_values.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.dl_model(input_values).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "            conf = probs[0, pred_idx].item()\n",
    "            label_name = self.id2label.get(pred_idx, \"Unknown\")\n",
    "\n",
    "            # 5. Calculate \"AI Probability\"\n",
    "            if pred_idx == 3: # \"Real\" label\n",
    "                ai_prob = 1.0 - conf\n",
    "            else: # Any AI label\n",
    "                ai_prob = conf\n",
    "\n",
    "            return round(ai_prob, 3), label_name\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: THE ENSEMBLE ORCHESTRATOR\n",
    "    # ==========================================================\n",
    "    def analyze(self, file_path):\n",
    "        # 1. Run Physics Brain\n",
    "        phys_score, _, phys_feats = self.get_physics_score(file_path)\n",
    "        \n",
    "        # 2. Run Deep Learning Brain\n",
    "        dl_score, dl_label = self.get_dl_score(file_path)\n",
    "\n",
    "        # 3. The VETO Logic (Max Suspicion)\n",
    "        # We trust whichever model is MORE suspicious of AI.\n",
    "        final_score = max(phys_score, dl_score)\n",
    "        \n",
    "        # 4. Generate Dynamic Explanation\n",
    "        explanations = []\n",
    "        \n",
    "        if final_score > 0.55:\n",
    "            classification = \"AI_GENERATED\"\n",
    "            \n",
    "            # Did DL catch it?\n",
    "            if dl_score > 0.55:\n",
    "                if dl_label != \"Real\":\n",
    "                    explanations.append(f\"Deep Learning detected artifacts consistent with '{dl_label}' generator\")\n",
    "                else:\n",
    "                    explanations.append(\"Deep Learning detected synthetic anomalies\")\n",
    "\n",
    "            # Did Physics catch it? (e.g., ElevenLabs)\n",
    "            if phys_score > 0.55:\n",
    "                p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                i_std = phys_feats.get('intensity_std', 0)\n",
    "                \n",
    "                if i_std < 0.05:\n",
    "                    explanations.append(f\"Intensity is unnaturally standardized (std: {i_std:.3f})\")\n",
    "                if p_cv < 0.20:\n",
    "                    explanations.append(f\"Pitch modulation is robotic (CV: {p_cv:.2f})\")\n",
    "                if not explanations: # Fallback\n",
    "                     explanations.append(\"Acoustic parameters lack natural human variability\")\n",
    "        else:\n",
    "            classification = \"HUMAN\"\n",
    "            explanations.append(\"Voice exhibits natural acoustic variability and lacks synthetic artifacts\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": classification,\n",
    "            \"confidenceScore\": final_score,\n",
    "            \"explanation\": \"; \".join(explanations),\n",
    "            \"debug\": {\n",
    "                \"Physics_Score\": phys_score,\n",
    "                \"DL_Score\": dl_score,\n",
    "                \"DL_Label\": dl_label\n",
    "            }\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    def __init__(self, model_path=\"wav2vec2_finetuned_model\"):\n",
    "        # --- 1. SETUP WAV2VEC2 (The Deep Learning Brain) ---\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Loading Wav2Vec2 model from '{model_path}' on {self.device}...\")\n",
    "        \n",
    "        try:\n",
    "            self.dl_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "            self.dl_model.to(self.device)\n",
    "            self.dl_model.eval()\n",
    "            self.dl_ready = True\n",
    "            print(\"✅ Wav2Vec2 Model Loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Wav2Vec2 Load Failed: {e}. Running in Physics-Only mode.\")\n",
    "            self.dl_ready = False\n",
    "\n",
    "        self.id2label = {\n",
    "            0: \"diffwave\", 1: \"melgan\", 2: \"parallel_wave_gan\", \n",
    "            3: \"Real\", 4: \"wavegrad\", 5: \"wavnet\", 6: \"wavernn\"\n",
    "        }\n",
    "\n",
    "        # --- 2. SETUP PHYSICS PARAMETERS (Tuned for Hackathon Sample) ---\n",
    "        # Adjusted thresholds to catch the 0.485 edge case\n",
    "        self.CV_AI_THRESHOLD = 0.20      # Raised from 0.15 to catch more \"semi-robotic\" voices\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32\n",
    "        self.INTENSITY_MIN_STD = 0.05    # Raised from 0.03 to catch slightly better volume fakes\n",
    "        self.INTENSITY_MAX_STD = 0.15 \n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: PHYSICS ENGINE\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        try:\n",
    "            # Load Audio (Native SR)\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Robust Pitch Tracking\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return 0.0, \"No voice detected\", {}\n",
    "\n",
    "            # Feature Extraction\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }\n",
    "\n",
    "            # Scoring Logic\n",
    "            i_score = self.get_linear_score(feats['intensity_std'], self.INTENSITY_MIN_STD, self.INTENSITY_MAX_STD)\n",
    "            p_score = self.get_linear_score(feats['pitch_cv'], self.CV_AI_THRESHOLD, self.CV_HUMAN_THRESHOLD)\n",
    "            s_score = self.get_linear_score(abs(feats['freq_skew']), 0.1, 1.0)\n",
    "\n",
    "            # Weights\n",
    "            W_INTENSITY = 0.40\n",
    "            W_PITCH = 0.40\n",
    "            W_SKEW = 0.20\n",
    "            \n",
    "            base_score = (i_score * W_INTENSITY) + (p_score * W_PITCH) + (s_score * W_SKEW)\n",
    "\n",
    "            # Synergy Bonus\n",
    "            if i_score > 0.4 and p_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: WAV2VEC2 ENGINE (Librosa Port of Repo Logic)\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        if not self.dl_ready: return 0.0, \"Model not loaded\"\n",
    "\n",
    "        target_sr = 16000\n",
    "        max_len = target_sr * 10 # 10 seconds\n",
    "\n",
    "        try:\n",
    "            # 1. Load with Librosa (Bypassing Torchaudio errors)\n",
    "            # IMPORTANT: mono=False so we can select the first channel like the Repo does\n",
    "            waveform_np, sr = librosa.load(audio_path, sr=target_sr, mono=False) \n",
    "\n",
    "            # 2. Handle Channels (Repo Logic: take waveform[0] if stereo)\n",
    "            # Librosa returns (n_channels, n_samples) if mono=False and stereo\n",
    "            # Librosa returns (n_samples,) if mono file\n",
    "            if waveform_np.ndim > 1:\n",
    "                waveform_np = waveform_np[0] # Take first channel (preserves phase better than averaging)\n",
    "            \n",
    "            # 3. Convert to Tensor\n",
    "            waveform = torch.tensor(waveform_np).unsqueeze(0) # Shape: (1, n_samples)\n",
    "\n",
    "            # 4. Pad/Truncate\n",
    "            if waveform.size(1) > max_len: \n",
    "                waveform = waveform[:, :max_len]\n",
    "            elif waveform.size(1) < max_len:\n",
    "                waveform = F.pad(waveform, (0, max_len - waveform.size(1)))\n",
    "\n",
    "            # 5. Predict\n",
    "            inputs = self.processor(\n",
    "                waveform.squeeze().numpy(), \n",
    "                sampling_rate=target_sr, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True\n",
    "            )\n",
    "            input_values = inputs.input_values.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.dl_model(input_values).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "            conf = probs[0, pred_idx].item()\n",
    "            label_name = self.id2label.get(pred_idx, \"Unknown\")\n",
    "\n",
    "            # 6. Calculate \"AI Probability\"\n",
    "            # Label 3 is \"Real\". If model says Real, AI Score is (1 - confidence).\n",
    "            if pred_idx == 3: \n",
    "                ai_prob = 1.0 - conf\n",
    "            else: # Any AI label\n",
    "                ai_prob = conf\n",
    "\n",
    "            return round(ai_prob, 3), label_name\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: THE ENSEMBLE ORCHESTRATOR\n",
    "    # ==========================================================\n",
    "    def analyze(self, file_path):\n",
    "        # 1. Run Physics Brain\n",
    "        phys_score, _, phys_feats = self.get_physics_score(file_path)\n",
    "        \n",
    "        # 2. Run Deep Learning Brain\n",
    "        dl_score, dl_label = self.get_dl_score(file_path)\n",
    "\n",
    "        # 3. The VETO Logic\n",
    "        final_score = max(phys_score, dl_score)\n",
    "        \n",
    "        # 4. Generate Explanation\n",
    "        explanations = []\n",
    "        \n",
    "        if final_score > 0.55:\n",
    "            classification = \"AI_GENERATED\"\n",
    "            \n",
    "            # Did DL catch it?\n",
    "            if dl_score > 0.55:\n",
    "                if dl_label != \"Real\":\n",
    "                    explanations.append(f\"Deep Learning detected artifacts consistent with '{dl_label}' generator\")\n",
    "                else:\n",
    "                    explanations.append(\"Deep Learning detected synthetic anomalies\")\n",
    "\n",
    "            # Did Physics catch it?\n",
    "            if phys_score > 0.55:\n",
    "                p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                i_std = phys_feats.get('intensity_std', 0)\n",
    "                \n",
    "                if i_std < 0.06: # Updated explanation logic\n",
    "                    explanations.append(f\"Intensity is unnaturally standardized (std: {i_std:.3f})\")\n",
    "                if p_cv < 0.22:\n",
    "                    explanations.append(f\"Pitch modulation is robotic (CV: {p_cv:.2f})\")\n",
    "                if not explanations:\n",
    "                     explanations.append(\"Acoustic parameters lack natural human variability\")\n",
    "        else:\n",
    "            classification = \"HUMAN\"\n",
    "            explanations.append(\"Voice exhibits natural acoustic variability and lacks synthetic artifacts\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"classification\": classification,\n",
    "            \"confidenceScore\": final_score,\n",
    "            \"explanation\": \"; \".join(explanations),\n",
    "            \"debug\": {\n",
    "                \"Physics_Score\": phys_score,\n",
    "                \"DL_Score\": dl_score,\n",
    "                \"DL_Label\": dl_label\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# 1. Ensure 'voiceguard_model' folder exists\n",
    "detector = HybridEnsembleDetector()\n",
    "\n",
    "# 2. Test\n",
    "result = detector.analyze(r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e97528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers torch torchaudio pydub\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "model_path = \"wav2vec2_finetuned_model\"  # Update this path\n",
    "target_sample_rate = 16000\n",
    "max_length = target_sample_rate * 10  # 10 seconds\n",
    "\n",
    "# Label mapping\n",
    "id2label = {\n",
    "    0: \"diffwave\",\n",
    "    1: \"melgan\",\n",
    "    2: \"parallel_wave_gan\",\n",
    "    3: \"Real\",\n",
    "    4: \"wavegrad\",\n",
    "    5: \"wavnet\",\n",
    "    6: \"wavernn\"\n",
    "}\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading model...\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "model.eval()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path):\n",
    "    \"\"\"Convert MP3 to WAV format in memory\"\"\"\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    # Export to WAV in memory\n",
    "    wav_io = BytesIO()\n",
    "    audio.export(wav_io, format=\"wav\")\n",
    "    wav_io.seek(0)\n",
    "    return wav_io\n",
    "\n",
    "def predict_audio(audio_path):\n",
    "    \"\"\"Predict if audio is real or AI-generated\"\"\"\n",
    "    try:\n",
    "        # Convert MP3 to WAV if needed\n",
    "        if audio_path.lower().endswith('.mp3'):\n",
    "            print(f\"Converting MP3 to WAV...\")\n",
    "            wav_io = convert_mp3_to_wav(audio_path)\n",
    "            waveform, sample_rate = torchaudio.load(wav_io)\n",
    "        else:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        print(f\"Original sample rate: {sample_rate} Hz\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sample_rate != target_sample_rate:\n",
    "            print(f\"Resampling to {target_sample_rate} Hz...\")\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=sample_rate, \n",
    "                new_freq=target_sample_rate\n",
    "            )\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Truncate or pad to fixed length\n",
    "        if waveform.size(1) > max_length:\n",
    "            waveform = waveform[:, :max_length]\n",
    "            print(f\"Truncated to {max_length/target_sample_rate} seconds\")\n",
    "        elif waveform.size(1) < max_length:\n",
    "            waveform = torch.nn.functional.pad(\n",
    "                waveform, \n",
    "                (0, max_length - waveform.size(1))\n",
    "            )\n",
    "            print(f\"Padded to {max_length/target_sample_rate} seconds\")\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.ndim > 1:\n",
    "            waveform = waveform[0]\n",
    "        \n",
    "        # Process audio\n",
    "        inputs = processor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=target_sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        input_values = inputs[\"input_values\"].to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_label].item()\n",
    "        \n",
    "        # Get class name\n",
    "        class_name = id2label.get(predicted_label, \"Unknown Class\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Prediction: {class_name}\")\n",
    "        print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"Label ID: {predicted_label}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show all probabilities\n",
    "        print(\"\\nAll class probabilities:\")\n",
    "        for label_id, prob in enumerate(probabilities[0].cpu().numpy()):\n",
    "            print(f\"  {id2label[label_id]}: {prob*100:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"class_name\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"label\": predicted_label,\n",
    "            \"all_probabilities\": {id2label[i]: float(p) for i, p in enumerate(probabilities[0].cpu().numpy())}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8752a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"wav2vec2_finetuned_model\"  # Update this path\n",
    "target_sample_rate = 16000\n",
    "max_length = target_sample_rate * 10  # 10 seconds\n",
    "\n",
    "# Label mapping\n",
    "id2label = {\n",
    "    0: \"diffwave\",\n",
    "    1: \"melgan\",\n",
    "    2: \"parallel_wave_gan\",\n",
    "    3: \"Real\",\n",
    "    4: \"wavegrad\",\n",
    "    5: \"wavnet\",\n",
    "    6: \"wavernn\"\n",
    "}\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading model...\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "model.eval()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "def predict_audio(audio_path):\n",
    "    \"\"\"Predict if audio is real or AI-generated\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nLoading audio file: {audio_path}\")\n",
    "        \n",
    "        # Load audio with librosa - directly at target sample rate\n",
    "        # Set sr=target_sample_rate to resample during loading\n",
    "        waveform_np, sample_rate = librosa.load(\n",
    "            audio_path, \n",
    "            sr=target_sample_rate,  # Resample to 16kHz during load\n",
    "            mono=True  # Convert to mono\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded at sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Audio duration: {len(waveform_np)/sample_rate:.2f} seconds\")\n",
    "        print(f\"Waveform shape: {waveform_np.shape}\")\n",
    "        \n",
    "        # Truncate or pad to fixed length (10 seconds)\n",
    "        if len(waveform_np) > max_length:\n",
    "            waveform_np = waveform_np[:max_length]\n",
    "            print(f\"Truncated to {max_length/target_sample_rate} seconds\")\n",
    "        elif len(waveform_np) < max_length:\n",
    "            # Pad with zeros\n",
    "            padding = max_length - len(waveform_np)\n",
    "            waveform_np = np.pad(waveform_np, (0, padding), mode='constant')\n",
    "            print(f\"Padded to {max_length/target_sample_rate} seconds\")\n",
    "        \n",
    "        print(f\"Final waveform shape: {waveform_np.shape}\")\n",
    "        \n",
    "        # Process audio with the processor\n",
    "        print(\"Processing audio with Wav2Vec2Processor...\")\n",
    "        inputs = processor(\n",
    "            waveform_np,\n",
    "            sampling_rate=target_sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        input_values = inputs[\"input_values\"].to(device)\n",
    "        print(f\"Input values shape: {input_values.shape}\")\n",
    "        \n",
    "        # Inference\n",
    "        print(\"Running inference...\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_label].item()\n",
    "        \n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits values: {logits}\")\n",
    "        \n",
    "        # Get class name\n",
    "        class_name = id2label.get(predicted_label, \"Unknown Class\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🎯 Prediction: {class_name}\")\n",
    "        print(f\"📊 Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"🏷️  Label ID: {predicted_label}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show all probabilities\n",
    "        print(\"\\n📈 All class probabilities:\")\n",
    "        for label_id, prob in enumerate(probabilities[0].cpu().numpy()):\n",
    "            bar = \"█\" * int(prob * 50)\n",
    "            print(f\"  {id2label[label_id]:20s}: {prob*100:5.2f}% {bar}\")\n",
    "        \n",
    "        return {\n",
    "            \"class_name\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"label\": predicted_label,\n",
    "            \"all_probabilities\": {id2label[i]: float(p) for i, p in enumerate(probabilities[0].cpu().numpy())}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ef76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded on cpu\n",
      "\n",
      "Model Configuration:\n",
      "Number of labels: 7\n",
      "Model type: wav2vec2\n",
      "\n",
      "============================================================\n",
      "Processing: clova.mp3\n",
      "============================================================\n",
      "Loaded at sample rate: 16000 Hz\n",
      "Audio duration: 14.08 seconds\n",
      "Audio min/max: -0.9114 / 0.6248\n",
      "Audio mean/std: -0.0000 / 0.1290\n",
      "✂️  Truncated to 10.0 seconds\n",
      "🔧 Normalized audio\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: parallel_wave_gan\n",
      "📊 Confidence: 99.99%\n",
      "🏷️  Label ID: 2\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  diffwave            :   0.00% \n",
      "  melgan              :   0.00% \n",
      "  parallel_wave_gan   :  99.99% █████████████████████████████████████████████████\n",
      "  Real                :   0.00% \n",
      "  wavegrad            :   0.00% \n",
      "  wavnet              :   0.00% \n",
      "  wavernn             :   0.00% \n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  diffwave            :  -0.7177\n",
      "  melgan              :  -2.9187\n",
      "  parallel_wave_gan   :   9.7021\n",
      "  Real                :  -0.3731\n",
      "  wavegrad            :  -3.0424\n",
      "  wavnet              :  -1.9304\n",
      "  wavernn             :  -1.5569\n",
      "\n",
      "\n",
      "============================================================\n",
      "BATCH TESTING - Testing multiple files\n",
      "{'filename': 'clova.mp3', 'class_name': 'parallel_wave_gan', 'confidence': 0.999900221824646, 'label': 2, 'all_probabilities': {'diffwave': 2.9834325687261298e-05, 'melgan': 3.3022249681380345e-06, 'parallel_wave_gan': 0.999900221824646, 'Real': 4.2108145862584934e-05, 'wavegrad': 2.918182872235775e-06, 'wavnet': 8.872155376593582e-06, 'wavernn': 1.2890001926280092e-05}, 'logits': {'diffwave': -0.717680811882019, 'melgan': -2.9187443256378174, 'parallel_wave_gan': 9.702070236206055, 'Real': -0.37309950590133667, 'wavegrad': -3.04237961769104, 'wavnet': -1.9304231405258179, 'wavernn': -1.5568881034851074}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"wav2vec2_finetuned_model\"  # Update this path\n",
    "target_sample_rate = 16000\n",
    "max_length = target_sample_rate * 10  # 10 seconds\n",
    "\n",
    "# Label mapping\n",
    "id2label = {\n",
    "    0: \"diffwave\",\n",
    "    1: \"melgan\",\n",
    "    2: \"parallel_wave_gan\",\n",
    "    3: \"Real\",\n",
    "    4: \"wavegrad\",\n",
    "    5: \"wavnet\",\n",
    "    6: \"wavernn\"\n",
    "}\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading model...\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "model.eval()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Check model configuration\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "\n",
    "def predict_audio(audio_path, normalize_audio=True):\n",
    "    \"\"\"Predict if audio is real or AI-generated\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load audio with librosa\n",
    "        waveform_np, sample_rate = librosa.load(\n",
    "            audio_path, \n",
    "            sr=target_sample_rate,\n",
    "            mono=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded at sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Audio duration: {len(waveform_np)/sample_rate:.2f} seconds\")\n",
    "        print(f\"Audio min/max: {waveform_np.min():.4f} / {waveform_np.max():.4f}\")\n",
    "        print(f\"Audio mean/std: {waveform_np.mean():.4f} / {waveform_np.std():.4f}\")\n",
    "        \n",
    "        # Truncate or pad to fixed length (10 seconds)\n",
    "        if len(waveform_np) > max_length:\n",
    "            waveform_np = waveform_np[:max_length]\n",
    "            print(f\"✂️  Truncated to {max_length/target_sample_rate} seconds\")\n",
    "        elif len(waveform_np) < max_length:\n",
    "            padding = max_length - len(waveform_np)\n",
    "            waveform_np = np.pad(waveform_np, (0, padding), mode='constant')\n",
    "            print(f\"➕ Padded to {max_length/target_sample_rate} seconds\")\n",
    "        \n",
    "        # Optional normalization (sometimes helps)\n",
    "        if normalize_audio:\n",
    "            waveform_np = waveform_np / (np.abs(waveform_np).max() + 1e-8)\n",
    "            print(f\"🔧 Normalized audio\")\n",
    "        \n",
    "        # Process audio with the processor\n",
    "        inputs = processor(\n",
    "            waveform_np,\n",
    "            sampling_rate=target_sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        input_values = inputs[\"input_values\"].to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_label].item()\n",
    "        \n",
    "        # Get class name\n",
    "        class_name = id2label.get(predicted_label, \"Unknown Class\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🎯 Prediction: {class_name}\")\n",
    "        print(f\"📊 Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"🏷️  Label ID: {predicted_label}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show all probabilities with better visualization\n",
    "        print(\"\\n📈 All class probabilities:\")\n",
    "        probs_list = []\n",
    "        for label_id, prob in enumerate(probabilities[0].cpu().numpy()):\n",
    "            bar_length = int(prob * 50)\n",
    "            bar = \"█\" * bar_length\n",
    "            label = id2label[label_id]\n",
    "            print(f\"  {label:20s}: {prob*100:6.2f}% {bar}\")\n",
    "            probs_list.append((label, prob))\n",
    "        \n",
    "        # Show raw logits for debugging\n",
    "        print(\"\\n🔍 Raw logits (before softmax):\")\n",
    "        for label_id, logit in enumerate(logits[0].cpu().numpy()):\n",
    "            print(f\"  {id2label[label_id]:20s}: {logit:8.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": os.path.basename(audio_path),\n",
    "            \"class_name\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"label\": predicted_label,\n",
    "            \"all_probabilities\": {id2label[i]: float(p) for i, p in enumerate(probabilities[0].cpu().numpy())},\n",
    "            \"logits\": {id2label[i]: float(l) for i, l in enumerate(logits[0].cpu().numpy())}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test single file\n",
    "audio_file = r\"clova.mp3\"\n",
    "result = predict_audio(audio_file)\n",
    "\n",
    "# Test multiple files to see if model gives different predictions\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"BATCH TESTING - Testing multiple files\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd062115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MelodyMachine Deepfake Detection model...\n",
      "✅ Model loaded successfully!\n",
      "Model loaded on cpu\n",
      "\n",
      "Model Configuration:\n",
      "Model type: wav2vec2\n",
      "Number of labels: 2\n",
      "Labels: {0: 'fake', 1: 'real'}\n",
      "\n",
      "\n",
      "============================================================\n",
      "BATCH TESTING - Multiple Files\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 48000 Hz\n",
      "Audio duration: 5.76 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 92160])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 100.00%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   0.00% \n",
      "  real                : 100.00% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -5.7409\n",
      "  real                :   5.2522\n",
      "\n",
      "============================================================\n",
      "Processing: medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 48000 Hz\n",
      "Audio duration: 3.26 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 52224])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 100.00%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   0.00% \n",
      "  real                : 100.00% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -5.7411\n",
      "  real                :   5.2515\n",
      "\n",
      "============================================================\n",
      "Processing: narration_20251210_232729.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 32.91 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 526629])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 95.12%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   4.88% ██\n",
      "  real                :  95.12% ███████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -1.5991\n",
      "  real                :   1.3712\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 8.86 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 141689])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 100.00%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   0.00% \n",
      "  real                : 100.00% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -5.7414\n",
      "  real                :   5.2516\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 8.67 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 138763])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 100.00%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   0.00% \n",
      "  real                : 100.00% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -5.7376\n",
      "  real                :   5.2474\n",
      "\n",
      "============================================================\n",
      "Processing: clova.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 24000 Hz\n",
      "Audio duration: 14.08 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input shape: torch.Size([1, 225251])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 99.99%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  fake                :   0.01% \n",
      "  real                :  99.99% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  fake                :  -4.6976\n",
      "  real                :   4.2557\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 SUMMARY OF ALL PREDICTIONS\n",
      "============================================================\n",
      "Filename                            Prediction           Confidence\n",
      "------------------------------------------------------------\n",
      "medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3 real                 100.00%\n",
      "medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3 real                 100.00%\n",
      "narration_20251210_232729.mp3       real                  95.12%\n",
      "voice_preview_kanika - soft, smooth and muffled.mp3 real                 100.00%\n",
      "voice_preview_faiq - standard, clear and neutral.mp3 real                 100.00%\n",
      "clova.mp3                           real                  99.99%\n",
      "\n",
      "============================================================\n",
      "📈 STATISTICS\n",
      "============================================================\n",
      "real: 6 files (100.0%)\n",
      "\n",
      "Average confidence: 99.18%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"Deepfake-audio-detection-V2\"  # Update to your cloned path\n",
    "target_sample_rate = 16000  # Most audio models use 16kHz\n",
    "\n",
    "# Load model and feature extractor\n",
    "print(\"Loading MelodyMachine Deepfake Detection model...\")\n",
    "try:\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_path, local_files_only=True)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_path, local_files_only=True)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"\\nTrying alternative loading methods...\")\n",
    "    # Alternative: try loading without local_files_only\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_path)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Check model configuration\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Model type: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n",
    "print(f\"Number of labels: {model.config.num_labels if hasattr(model.config, 'num_labels') else 'Unknown'}\")\n",
    "if hasattr(model.config, 'id2label'):\n",
    "    print(f\"Labels: {model.config.id2label}\")\n",
    "\n",
    "def predict_deepfake(audio_path):\n",
    "    \"\"\"Predict if audio is deepfake/AI-generated or real\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load audio\n",
    "        print(\"Loading audio file...\")\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "        \n",
    "        print(f\"Original sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Audio duration: {len(waveform)/sample_rate:.2f} seconds\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sample_rate != target_sample_rate:\n",
    "            print(f\"Resampling to {target_sample_rate} Hz...\")\n",
    "            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=target_sample_rate)\n",
    "            sample_rate = target_sample_rate\n",
    "        \n",
    "        # Process audio with feature extractor\n",
    "        print(\"Extracting features...\")\n",
    "        inputs = feature_extractor(\n",
    "            waveform,\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        print(f\"Input shape: {inputs['input_values'].shape if 'input_values' in inputs else 'N/A'}\")\n",
    "        \n",
    "        # Inference\n",
    "        print(\"Running inference...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        # Get label names\n",
    "        if hasattr(model.config, 'id2label'):\n",
    "            id2label = model.config.id2label\n",
    "            class_name = id2label.get(predicted_class, f\"Class {predicted_class}\")\n",
    "        else:\n",
    "            # Default labels if not in config\n",
    "            id2label = {0: \"Real/Bonafide\", 1: \"Fake/Deepfake\"}\n",
    "            class_name = id2label.get(predicted_class, f\"Class {predicted_class}\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🎯 Prediction: {class_name}\")\n",
    "        print(f\"📊 Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"🏷️  Class ID: {predicted_class}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show all probabilities\n",
    "        print(\"\\n📈 All class probabilities:\")\n",
    "        for class_id in range(probabilities.shape[1]):\n",
    "            prob = probabilities[0, class_id].item()\n",
    "            label = id2label.get(class_id, f\"Class {class_id}\")\n",
    "            bar = \"█\" * int(prob * 50)\n",
    "            print(f\"  {label:20s}: {prob*100:6.2f}% {bar}\")\n",
    "        \n",
    "        # Show raw logits\n",
    "        print(\"\\n🔍 Raw logits (before softmax):\")\n",
    "        for class_id, logit in enumerate(logits[0].cpu().numpy()):\n",
    "            label = id2label.get(class_id, f\"Class {class_id}\")\n",
    "            print(f\"  {label:20s}: {logit:8.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": os.path.basename(audio_path),\n",
    "            \"prediction\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"class_id\": predicted_class,\n",
    "            \"all_probabilities\": {\n",
    "                id2label.get(i, f\"Class {i}\"): float(probabilities[0, i].cpu())\n",
    "                for i in range(probabilities.shape[1])\n",
    "            },\n",
    "            \"logits\": {\n",
    "                id2label.get(i, f\"Class {i}\"): float(logits[0, i].cpu())\n",
    "                for i in range(logits.shape[1])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test multiple files\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"BATCH TESTING - Multiple Files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "audio_files = [\n",
    "    r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "    r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\",\n",
    "    r\"narration_20251210_232729.mp3\",\n",
    "    r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "    r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "    r\"clova.mp3\"\n",
    "    # Add more files here\n",
    "]\n",
    "\n",
    "results = []\n",
    "for audio_file in audio_files:\n",
    "    if os.path.exists(audio_file):\n",
    "        result = predict_deepfake(audio_file)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️  File not found: {audio_file}\")\n",
    "\n",
    "# Summary Report\n",
    "if results:\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"📊 SUMMARY OF ALL PREDICTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Filename':<35} {'Prediction':<20} {'Confidence':<10}\")\n",
    "    print(\"-\"*60)\n",
    "    for r in results:\n",
    "        print(f\"{r['filename']:<35} {r['prediction']:<20} {r['confidence']*100:>6.2f}%\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count predictions\n",
    "    from collections import Counter\n",
    "    prediction_counts = Counter([r['prediction'] for r in results])\n",
    "    for pred, count in prediction_counts.items():\n",
    "        print(f\"{pred}: {count} files ({count/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Average confidence\n",
    "    avg_confidence = np.mean([r['confidence'] for r in results])\n",
    "    print(f\"\\nAverage confidence: {avg_confidence*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e2be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "✅ Model loaded on cpu\n",
      "\n",
      "============================================================\n",
      "HYBRID DEEPFAKE DETECTION - BATCH TESTING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\n",
      "============================================================\n",
      "Duration: 5.76s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 84.38%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (100.00%)\n",
      "🔬 Acoustic AI Score: 15.62%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1845.78 (AI if < 300)\n",
      "  ZCR Regularity: 0.2322 (AI if < 0.06)\n",
      "  MFCC Variation: 69.00 (AI if < 20)\n",
      "  Harmonic Ratio: 0.316 (AI if > 0.80)\n",
      "  RMS Variance: 0.0029 (AI if < 0.01)\n",
      "\n",
      "============================================================\n",
      "Processing: medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\n",
      "============================================================\n",
      "Duration: 3.26s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 83.55%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (100.00%)\n",
      "🔬 Acoustic AI Score: 16.45%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1125.55 (AI if < 300)\n",
      "  ZCR Regularity: 0.1036 (AI if < 0.06)\n",
      "  MFCC Variation: 70.86 (AI if < 20)\n",
      "  Harmonic Ratio: 0.442 (AI if > 0.80)\n",
      "  RMS Variance: 0.0023 (AI if < 0.01)\n",
      "\n",
      "============================================================\n",
      "Processing: narration_20251210_232729.mp3\n",
      "============================================================\n",
      "Duration: 32.91s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 82.61%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (95.12%)\n",
      "🔬 Acoustic AI Score: 17.39%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1119.75 (AI if < 300)\n",
      "  ZCR Regularity: 0.1292 (AI if < 0.06)\n",
      "  MFCC Variation: 83.71 (AI if < 20)\n",
      "  Harmonic Ratio: 0.431 (AI if > 0.80)\n",
      "  RMS Variance: 0.0014 (AI if < 0.01)\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "============================================================\n",
      "Duration: 8.86s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 83.44%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (100.00%)\n",
      "🔬 Acoustic AI Score: 16.56%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1019.18 (AI if < 300)\n",
      "  ZCR Regularity: 0.1407 (AI if < 0.06)\n",
      "  MFCC Variation: 83.35 (AI if < 20)\n",
      "  Harmonic Ratio: 0.541 (AI if > 0.80)\n",
      "  RMS Variance: 0.0021 (AI if < 0.01)\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "============================================================\n",
      "Duration: 8.67s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 83.50%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (100.00%)\n",
      "🔬 Acoustic AI Score: 16.50%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1014.70 (AI if < 300)\n",
      "  ZCR Regularity: 0.0953 (AI if < 0.06)\n",
      "  MFCC Variation: 80.53 (AI if < 20)\n",
      "  Harmonic Ratio: 0.620 (AI if > 0.80)\n",
      "  RMS Variance: 0.0021 (AI if < 0.01)\n",
      "\n",
      "============================================================\n",
      "Processing: clova.mp3\n",
      "============================================================\n",
      "Duration: 14.08s | Sample rate: 16000 Hz\n",
      "\n",
      "==================================================\n",
      "🎯 FINAL VERDICT: Real Human Voice\n",
      "📊 Confidence: 89.06%\n",
      "🔍 Detection Method: Combined Analysis\n",
      "==================================================\n",
      "\n",
      "📱 Model Prediction: real (99.99%)\n",
      "🔬 Acoustic AI Score: 10.94%\n",
      "\n",
      "🎼 Key Acoustic Indicators:\n",
      "  Spectral Consistency: 1336.32 (AI if < 300)\n",
      "  ZCR Regularity: 0.1370 (AI if < 0.06)\n",
      "  MFCC Variation: 64.90 (AI if < 20)\n",
      "  Harmonic Ratio: 0.704 (AI if > 0.80)\n",
      "  RMS Variance: 0.0042 (AI if < 0.01)\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 FINAL SUMMARY\n",
      "============================================================\n",
      "Filename                                           Prediction                Confidence   Method\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "medieval-gamer-voice-darkness-hunts-us-what-you... Real Human Voice           84.38%      Combined Analysis\n",
      "medieval-gamer-voice-you-can-view-our-website-a... Real Human Voice           83.55%      Combined Analysis\n",
      "narration_20251210_232729.mp3                      Real Human Voice           82.61%      Combined Analysis\n",
      "voice_preview_kanika - soft, smooth and muffled... Real Human Voice           83.44%      Combined Analysis\n",
      "voice_preview_faiq - standard, clear and neutra... Real Human Voice           83.50%      Combined Analysis\n",
      "clova.mp3                                          Real Human Voice           89.06%      Combined Analysis\n",
      "\n",
      "============================================================\n",
      "📈 DETECTION STATISTICS\n",
      "============================================================\n",
      "Real Human Voice: 6 files (100.0%)\n",
      "\n",
      "Detected by Acoustic Analysis: 0\n",
      "Detected by Model: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "from scipy import signal\n",
    "from scipy.stats import kurtosis, skew\n",
    "import os\n",
    "\n",
    "class HybridDeepfakeDetector:\n",
    "    \"\"\"\n",
    "    Combines model predictions with acoustic analysis to detect modern AI voices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        print(\"Loading model...\")\n",
    "        self.model = AutoModelForAudioClassification.from_pretrained(model_path, local_files_only=True)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model.eval()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"✅ Model loaded on {self.device}\")\n",
    "    \n",
    "    def extract_advanced_features(self, waveform, sr=16000):\n",
    "        \"\"\"Extract acoustic features that reveal AI generation\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Spectral Features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=waveform, sr=sr)[0]\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroids)\n",
    "        features['spectral_centroid_var'] = np.var(spectral_centroids)\n",
    "        \n",
    "        # 2. Zero Crossing Rate (AI has more regular patterns)\n",
    "        zcr = librosa.feature.zero_crossing_rate(waveform)[0]\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        features['zcr_var'] = np.var(zcr)\n",
    "        \n",
    "        # 3. MFCC Statistics (AI has less natural variation)\n",
    "        mfccs = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=20)\n",
    "        features['mfcc_mean'] = np.mean(mfccs)\n",
    "        features['mfcc_std'] = np.std(mfccs)\n",
    "        features['mfcc_var'] = np.var(mfccs)\n",
    "        features['mfcc_kurtosis'] = np.mean([kurtosis(mfcc) for mfcc in mfccs])\n",
    "        features['mfcc_skewness'] = np.mean([skew(mfcc) for mfcc in mfccs])\n",
    "        \n",
    "        # 4. Spectral Rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=waveform, sr=sr)[0]\n",
    "        features['rolloff_mean'] = np.mean(rolloff)\n",
    "        features['rolloff_std'] = np.std(rolloff)\n",
    "        \n",
    "        # 5. Spectral Bandwidth\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0]\n",
    "        features['bandwidth_mean'] = np.mean(bandwidth)\n",
    "        features['bandwidth_std'] = np.std(bandwidth)\n",
    "        \n",
    "        # 6. RMS Energy\n",
    "        rms = librosa.feature.rms(y=waveform)[0]\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        features['rms_var'] = np.var(rms)\n",
    "        \n",
    "        # 7. Harmonic-Percussive Source Separation\n",
    "        harmonic, percussive = librosa.effects.hpss(waveform)\n",
    "        features['harmonic_ratio'] = np.sum(np.abs(harmonic)) / (np.sum(np.abs(waveform)) + 1e-8)\n",
    "        features['percussive_ratio'] = np.sum(np.abs(percussive)) / (np.sum(np.abs(waveform)) + 1e-8)\n",
    "        \n",
    "        # 8. Spectral Contrast (AI often has smoother contrast)\n",
    "        contrast = librosa.feature.spectral_contrast(y=waveform, sr=sr)\n",
    "        features['contrast_mean'] = np.mean(contrast)\n",
    "        features['contrast_std'] = np.std(contrast)\n",
    "        \n",
    "        # 9. Chroma Features\n",
    "        chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
    "        features['chroma_mean'] = np.mean(chroma)\n",
    "        features['chroma_std'] = np.std(chroma)\n",
    "        \n",
    "        # 10. Temporal Features\n",
    "        features['duration'] = len(waveform) / sr\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_ai_score(self, features):\n",
    "        \"\"\"\n",
    "        Calculate AI likelihood based on acoustic signatures\n",
    "        Modern TTS (ElevenLabs, OpenAI, etc.) characteristics:\n",
    "        - Very consistent spectral features (low variance)\n",
    "        - Regular zero-crossing patterns\n",
    "        - Unnaturally smooth harmonic structure\n",
    "        - Less MFCC variation\n",
    "        - High harmonic-to-noise ratio\n",
    "        \"\"\"\n",
    "        ai_indicators = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        # 1. Spectral Centroid Consistency (weight: 3)\n",
    "        # AI voices have very stable spectral centroids\n",
    "        if features['spectral_centroid_std'] < 300:\n",
    "            ai_indicators += 3 * (1 - features['spectral_centroid_std'] / 300)\n",
    "        total_weight += 3\n",
    "        \n",
    "        # 2. Zero Crossing Rate Regularity (weight: 2)\n",
    "        # AI has very regular ZCR patterns\n",
    "        if features['zcr_std'] < 0.06:\n",
    "            ai_indicators += 2 * (1 - features['zcr_std'] / 0.06)\n",
    "        total_weight += 2\n",
    "        \n",
    "        # 3. MFCC Variation (weight: 4)\n",
    "        # Human voices have more MFCC variation\n",
    "        if features['mfcc_std'] < 20:\n",
    "            ai_indicators += 4 * (1 - features['mfcc_std'] / 20)\n",
    "        total_weight += 4\n",
    "        \n",
    "        # 4. RMS Energy Consistency (weight: 2)\n",
    "        # AI has very consistent energy levels\n",
    "        if features['rms_var'] < 0.01:\n",
    "            ai_indicators += 2 * (1 - features['rms_var'] / 0.01)\n",
    "        total_weight += 2\n",
    "        \n",
    "        # 5. Harmonic Ratio (weight: 3)\n",
    "        # AI voices are \"too clean\" - very high harmonic ratio\n",
    "        if features['harmonic_ratio'] > 0.80:\n",
    "            ai_indicators += 3 * ((features['harmonic_ratio'] - 0.80) / 0.20)\n",
    "        total_weight += 3\n",
    "        \n",
    "        # 6. Spectral Bandwidth Consistency (weight: 2)\n",
    "        # AI has more stable bandwidth\n",
    "        if features['bandwidth_std'] < 400:\n",
    "            ai_indicators += 2 * (1 - features['bandwidth_std'] / 400)\n",
    "        total_weight += 2\n",
    "        \n",
    "        # 7. Spectral Contrast Smoothness (weight: 2)\n",
    "        # AI has smoother spectral contrast\n",
    "        if features['contrast_std'] < 5:\n",
    "            ai_indicators += 2 * (1 - features['contrast_std'] / 5)\n",
    "        total_weight += 2\n",
    "        \n",
    "        # 8. MFCC Kurtosis (weight: 2)\n",
    "        # AI often has different distribution shapes\n",
    "        if abs(features['mfcc_kurtosis']) < 1.5:\n",
    "            ai_indicators += 2 * (1 - abs(features['mfcc_kurtosis']) / 1.5)\n",
    "        total_weight += 2\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        ai_score = ai_indicators / total_weight\n",
    "        \n",
    "        return ai_score\n",
    "    \n",
    "    def predict(self, audio_path, show_details=True):\n",
    "        \"\"\"Combined prediction using model + acoustic analysis\"\"\"\n",
    "        try:\n",
    "            if show_details:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "                print('='*60)\n",
    "            \n",
    "            # Load audio\n",
    "            waveform, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "            \n",
    "            if show_details:\n",
    "                print(f\"Duration: {len(waveform)/sr:.2f}s | Sample rate: {sr} Hz\")\n",
    "            \n",
    "            # 1. Model Prediction\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=sr,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                model_pred_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                model_confidence = probabilities[0, model_pred_class].item()\n",
    "            \n",
    "            id2label = self.model.config.id2label\n",
    "            model_prediction = id2label[model_pred_class]\n",
    "            \n",
    "            # 2. Acoustic Analysis\n",
    "            features = self.extract_advanced_features(waveform, sr)\n",
    "            acoustic_ai_score = self.calculate_ai_score(features)\n",
    "            \n",
    "            # 3. Combined Decision\n",
    "            # If model says real BUT acoustic score is high, override to AI\n",
    "            if model_prediction == \"real\" and acoustic_ai_score > 0.55:\n",
    "                final_prediction = \"AI Generated (Modern TTS)\"\n",
    "                final_confidence = acoustic_ai_score\n",
    "                detection_method = \"Acoustic Analysis\"\n",
    "            elif model_prediction == \"fake\":\n",
    "                final_prediction = \"AI Generated (Classic TTS)\"\n",
    "                final_confidence = model_confidence\n",
    "                detection_method = \"Model Detection\"\n",
    "            else:\n",
    "                final_prediction = \"Real Human Voice\"\n",
    "                final_confidence = 1 - acoustic_ai_score\n",
    "                detection_method = \"Combined Analysis\"\n",
    "            \n",
    "            # Display results\n",
    "            if show_details:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(f\"🎯 FINAL VERDICT: {final_prediction}\")\n",
    "                print(f\"📊 Confidence: {final_confidence*100:.2f}%\")\n",
    "                print(f\"🔍 Detection Method: {detection_method}\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                print(f\"\\n📱 Model Prediction: {model_prediction} ({model_confidence*100:.2f}%)\")\n",
    "                print(f\"🔬 Acoustic AI Score: {acoustic_ai_score*100:.2f}%\")\n",
    "                \n",
    "                print(\"\\n🎼 Key Acoustic Indicators:\")\n",
    "                print(f\"  Spectral Consistency: {features['spectral_centroid_std']:.2f} (AI if < 300)\")\n",
    "                print(f\"  ZCR Regularity: {features['zcr_std']:.4f} (AI if < 0.06)\")\n",
    "                print(f\"  MFCC Variation: {features['mfcc_std']:.2f} (AI if < 20)\")\n",
    "                print(f\"  Harmonic Ratio: {features['harmonic_ratio']:.3f} (AI if > 0.80)\")\n",
    "                print(f\"  RMS Variance: {features['rms_var']:.4f} (AI if < 0.01)\")\n",
    "            \n",
    "            return {\n",
    "                \"filename\": os.path.basename(audio_path),\n",
    "                \"final_prediction\": final_prediction,\n",
    "                \"final_confidence\": final_confidence,\n",
    "                \"model_prediction\": model_prediction,\n",
    "                \"model_confidence\": model_confidence,\n",
    "                \"acoustic_ai_score\": acoustic_ai_score,\n",
    "                \"detection_method\": detection_method,\n",
    "                \"features\": features\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# Initialize detector\n",
    "model_path = r\"Deepfake-audio-detection-V2\"\n",
    "detector = HybridDeepfakeDetector(model_path)\n",
    "\n",
    "# Test files\n",
    "audio_files = [\n",
    "    r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "    r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\",\n",
    "    r\"narration_20251210_232729.mp3\",\n",
    "    r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "    r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "    r\"clova.mp3\"\n",
    "    # Add more files here\n",
    "]\n",
    "\n",
    "# Batch testing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYBRID DEEPFAKE DETECTION - BATCH TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for audio_file in audio_files:\n",
    "    if os.path.exists(audio_file):\n",
    "        result = detector.predict(audio_file, show_details=True)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️  File not found: {audio_file}\")\n",
    "\n",
    "# Summary\n",
    "if results:\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"📊 FINAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Filename':<50} {'Prediction':<25} {'Confidence':<12} {'Method'}\")\n",
    "    print(\"-\"*120)\n",
    "    for r in results:\n",
    "        filename = r['filename'][:47] + \"...\" if len(r['filename']) > 50 else r['filename']\n",
    "        pred = r['final_prediction'][:22] + \"...\" if len(r['final_prediction']) > 25 else r['final_prediction']\n",
    "        print(f\"{filename:<50} {pred:<25} {r['final_confidence']*100:>6.2f}%      {r['detection_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 DETECTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from collections import Counter\n",
    "    prediction_counts = Counter([r['final_prediction'] for r in results])\n",
    "    for pred, count in prediction_counts.items():\n",
    "        print(f\"{pred}: {count} files ({count/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Show which were detected by acoustic vs model\n",
    "    acoustic_detections = sum(1 for r in results if \"Acoustic\" in r['detection_method'])\n",
    "    model_detections = sum(1 for r in results if \"Model\" in r['detection_method'])\n",
    "    print(f\"\\nDetected by Acoustic Analysis: {acoustic_detections}\")\n",
    "    print(f\"Detected by Model: {model_detections}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01952422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gustking Deepfake Detection model...\n",
      "✅ Loaded with AutoFeatureExtractor\n",
      "✅ Model loaded successfully!\n",
      "Model loaded on cpu\n",
      "\n",
      "Model Configuration:\n",
      "Model type: wav2vec2\n",
      "Number of labels: 2\n",
      "Labels: {0: 'real', 1: 'fake'}\n",
      "\n",
      "\n",
      "============================================================\n",
      "BATCH TESTING - Multiple Files\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: narration_20251210_232729.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 32.91 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 526629])\n",
      "Input 'attention_mask' shape: torch.Size([1, 526629])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 87.40%\n",
      "🏷️  Class ID: 0\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  87.40% ███████████████████████████████████████████\n",
      "  fake                :  12.60% ██████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :   0.9566\n",
      "  fake                :  -0.9800\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 8.86 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 141689])\n",
      "Input 'attention_mask' shape: torch.Size([1, 141689])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 89.79%\n",
      "🏷️  Class ID: 0\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  89.79% ████████████████████████████████████████████\n",
      "  fake                :  10.21% █████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :   1.0697\n",
      "  fake                :  -1.1044\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 8.67 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 138763])\n",
      "Input 'attention_mask' shape: torch.Size([1, 138763])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: fake\n",
      "📊 Confidence: 67.13%\n",
      "🏷️  Class ID: 1\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  32.87% ████████████████\n",
      "  fake                :  67.13% █████████████████████████████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :  -0.3533\n",
      "  fake                :   0.3608\n",
      "\n",
      "============================================================\n",
      "Processing: voice_preview_tarini - soft, cheerful and expressive.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 7.60 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 121627])\n",
      "Input 'attention_mask' shape: torch.Size([1, 121627])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 87.05%\n",
      "🏷️  Class ID: 0\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  87.05% ███████████████████████████████████████████\n",
      "  fake                :  12.95% ██████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :   0.9385\n",
      "  fake                :  -0.9665\n",
      "\n",
      "============================================================\n",
      "Processing: clova.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 24000 Hz\n",
      "Audio duration: 14.08 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 225251])\n",
      "Input 'attention_mask' shape: torch.Size([1, 225251])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 76.47%\n",
      "🏷️  Class ID: 0\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  76.47% ██████████████████████████████████████\n",
      "  fake                :  23.53% ███████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :   0.5748\n",
      "  fake                :  -0.6039\n",
      "\n",
      "============================================================\n",
      "Processing: sample voice 1.mp3\n",
      "============================================================\n",
      "Loading audio file...\n",
      "Original sample rate: 44100 Hz\n",
      "Audio duration: 23.59 seconds\n",
      "Resampling to 16000 Hz...\n",
      "Extracting features...\n",
      "Input 'input_values' shape: torch.Size([1, 377418])\n",
      "Input 'attention_mask' shape: torch.Size([1, 377418])\n",
      "Running inference...\n",
      "\n",
      "==================================================\n",
      "🎯 Prediction: real\n",
      "📊 Confidence: 77.26%\n",
      "🏷️  Class ID: 0\n",
      "==================================================\n",
      "\n",
      "📈 All class probabilities:\n",
      "  real                :  77.26% ██████████████████████████████████████\n",
      "  fake                :  22.74% ███████████\n",
      "\n",
      "🔍 Raw logits (before softmax):\n",
      "  real                :   0.6005\n",
      "  fake                :  -0.6227\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 SUMMARY OF ALL PREDICTIONS\n",
      "============================================================\n",
      "Filename                                           Prediction           Confidence\n",
      "--------------------------------------------------------------------------------\n",
      "narration_20251210_232729.mp3                      real                  87.40%\n",
      "voice_preview_kanika - soft, smooth and muffled... real                  89.79%\n",
      "voice_preview_faiq - standard, clear and neutra... fake                  67.13%\n",
      "voice_preview_tarini - soft, cheerful and expre... real                  87.05%\n",
      "clova.mp3                                          real                  76.47%\n",
      "sample voice 1.mp3                                 real                  77.26%\n",
      "\n",
      "============================================================\n",
      "📈 STATISTICS\n",
      "============================================================\n",
      "real: 5 files (83.3%)\n",
      "fake: 1 files (16.7%)\n",
      "\n",
      "Average confidence: 80.85%\n",
      "\n",
      "✅ Detected as REAL (5):\n",
      "  - narration_20251210_232729.mp3\n",
      "  - voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "  - voice_preview_tarini - soft, cheerful and expressive.mp3\n",
      "  - clova.mp3\n",
      "  - sample voice 1.mp3\n",
      "\n",
      "❌ Detected as FAKE/AI (1):\n",
      "  - voice_preview_faiq - standard, clear and neutral.mp3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, Wav2Vec2Processor\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"wav2vec2-large-xlsr-deepfake-audio-classification\"  # Update path\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading Gustking Deepfake Detection model...\")\n",
    "try:\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_path, local_files_only=True)\n",
    "    \n",
    "    # Try different processor types\n",
    "    try:\n",
    "        processor = AutoFeatureExtractor.from_pretrained(model_path, local_files_only=True)\n",
    "        print(\"✅ Loaded with AutoFeatureExtractor\")\n",
    "    except:\n",
    "        processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
    "        print(\"✅ Loaded with Wav2Vec2Processor\")\n",
    "    \n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Check model configuration\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Model type: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n",
    "print(f\"Number of labels: {model.config.num_labels if hasattr(model.config, 'num_labels') else 'Unknown'}\")\n",
    "if hasattr(model.config, 'id2label'):\n",
    "    print(f\"Labels: {model.config.id2label}\")\n",
    "else:\n",
    "    print(\"Labels: Not found in config (will use default)\")\n",
    "\n",
    "def predict_deepfake(audio_path):\n",
    "    \"\"\"Predict if audio is deepfake/AI-generated or real\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load audio\n",
    "        print(\"Loading audio file...\")\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "        \n",
    "        print(f\"Original sample rate: {sample_rate} Hz\")\n",
    "        print(f\"Audio duration: {len(waveform)/sample_rate:.2f} seconds\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sample_rate != target_sample_rate:\n",
    "            print(f\"Resampling to {target_sample_rate} Hz...\")\n",
    "            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=target_sample_rate)\n",
    "            sample_rate = target_sample_rate\n",
    "        \n",
    "        # Process audio with processor\n",
    "        print(\"Extracting features...\")\n",
    "        inputs = processor(\n",
    "            waveform,\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Convert BatchFeature to dict and move to device\n",
    "        if hasattr(inputs, 'data'):\n",
    "            # It's a BatchFeature object\n",
    "            input_dict = dict(inputs.data)\n",
    "        elif isinstance(inputs, dict):\n",
    "            input_dict = inputs\n",
    "        else:\n",
    "            # Try to convert to dict\n",
    "            input_dict = {k: v for k, v in inputs.items()}\n",
    "        \n",
    "        # Move to device\n",
    "        input_dict = {key: val.to(device) for key, val in input_dict.items()}\n",
    "        \n",
    "        # Print shape info\n",
    "        for key, val in input_dict.items():\n",
    "            print(f\"Input '{key}' shape: {val.shape}\")\n",
    "        \n",
    "        # Inference\n",
    "        print(\"Running inference...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_dict)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        # Get label names\n",
    "        if hasattr(model.config, 'id2label'):\n",
    "            id2label = model.config.id2label\n",
    "        else:\n",
    "            # Default labels\n",
    "            num_labels = probabilities.shape[1]\n",
    "            if num_labels == 2:\n",
    "                id2label = {0: \"Bonafide/Real\", 1: \"Spoof/Fake\"}\n",
    "            else:\n",
    "                id2label = {i: f\"Class {i}\" for i in range(num_labels)}\n",
    "        \n",
    "        class_name = id2label.get(predicted_class, f\"Class {predicted_class}\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🎯 Prediction: {class_name}\")\n",
    "        print(f\"📊 Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"🏷️  Class ID: {predicted_class}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show all probabilities\n",
    "        print(\"\\n📈 All class probabilities:\")\n",
    "        for class_id in range(probabilities.shape[1]):\n",
    "            prob = probabilities[0, class_id].item()\n",
    "            label = id2label.get(class_id, f\"Class {class_id}\")\n",
    "            bar = \"█\" * int(prob * 50)\n",
    "            print(f\"  {label:20s}: {prob*100:6.2f}% {bar}\")\n",
    "        \n",
    "        # Show raw logits\n",
    "        print(\"\\n🔍 Raw logits (before softmax):\")\n",
    "        for class_id, logit in enumerate(logits[0].cpu().numpy()):\n",
    "            label = id2label.get(class_id, f\"Class {class_id}\")\n",
    "            print(f\"  {label:20s}: {logit:8.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": os.path.basename(audio_path),\n",
    "            \"prediction\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"class_id\": predicted_class,\n",
    "            \"all_probabilities\": {\n",
    "                id2label.get(i, f\"Class {i}\"): float(probabilities[0, i].cpu())\n",
    "                for i in range(probabilities.shape[1])\n",
    "            },\n",
    "            \"logits\": {\n",
    "                id2label.get(i, f\"Class {i}\"): float(logits[0, i].cpu())\n",
    "                for i in range(logits.shape[1])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Test multiple files\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"BATCH TESTING - Multiple Files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "audio_files = [\n",
    "    r\"narration_20251210_232729.mp3\",\n",
    "    r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "    r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "    r\"voice_preview_tarini - soft, cheerful and expressive.mp3\",\n",
    "    r\"clova.mp3\",\n",
    "    r\"sample voice 1.mp3\"\n",
    "    # Add more files here\n",
    "]\n",
    "results = []\n",
    "for audio_file in audio_files:\n",
    "    if os.path.exists(audio_file):\n",
    "        result = predict_deepfake(audio_file)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️  File not found: {audio_file}\")\n",
    "\n",
    "# Summary Report\n",
    "if results:\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"📊 SUMMARY OF ALL PREDICTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Filename':<50} {'Prediction':<20} {'Confidence':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    for r in results:\n",
    "        filename = r['filename'][:47] + \"...\" if len(r['filename']) > 50 else r['filename']\n",
    "        print(f\"{filename:<50} {r['prediction']:<20} {r['confidence']*100:>6.2f}%\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from collections import Counter\n",
    "    prediction_counts = Counter([r['prediction'] for r in results])\n",
    "    for pred, count in prediction_counts.items():\n",
    "        print(f\"{pred}: {count} files ({count/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    avg_confidence = np.mean([r['confidence'] for r in results])\n",
    "    print(f\"\\nAverage confidence: {avg_confidence*100:.2f}%\")\n",
    "    \n",
    "    # Group by prediction type\n",
    "    real_files = [r['filename'] for r in results if 'real' in r['prediction'].lower() or 'bonafide' in r['prediction'].lower()]\n",
    "    fake_files = [r['filename'] for r in results if 'fake' in r['prediction'].lower() or 'spoof' in r['prediction'].lower()]\n",
    "    \n",
    "    if real_files:\n",
    "        print(f\"\\n✅ Detected as REAL ({len(real_files)}):\")\n",
    "        for f in real_files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    if fake_files:\n",
    "        print(f\"\\n❌ Detected as FAKE/AI ({len(fake_files)}):\")\n",
    "        for f in fake_files:\n",
    "            print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100f1872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading garystafford/wav2vec2-deepfake-voice-detector model...\n",
      "✅ Model loaded from local path: wav2vec2-deepfake-voice-detector\n",
      "Model on cpu\n",
      "\n",
      "Model Configuration:\n",
      "Labels: {0: 'real', 1: 'fake'}\n",
      "Number of labels: 2\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BATCH TESTING - Multiple Files\n",
      "======================================================================\n",
      "======================================================================\n",
      "Processing: medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\n",
      "======================================================================\n",
      "Audio loaded: 5.76s duration @ 16000 Hz\n",
      "Audio shape: (92160,)\n",
      "Input 'input_values' shape: torch.Size([1, 92160])\n",
      "Input 'attention_mask' shape: torch.Size([1, 92160])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: REAL/HUMAN\n",
      "📊 Confidence: 86.10%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:     86.10% ███████████████████████████████████████████\n",
      "  Fake/Deepfake:  13.90% ██████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:    0.8975\n",
      "  Fake:   -0.9257\n",
      "======================================================================\n",
      "Processing: medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\n",
      "======================================================================\n",
      "Audio loaded: 3.26s duration @ 16000 Hz\n",
      "Audio shape: (52224,)\n",
      "Input 'input_values' shape: torch.Size([1, 52224])\n",
      "Input 'attention_mask' shape: torch.Size([1, 52224])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 99.24%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      0.76% \n",
      "  Fake/Deepfake:  99.24% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -2.4096\n",
      "  Fake:    2.4667\n",
      "======================================================================\n",
      "Processing: narration_20251210_232729.mp3\n",
      "======================================================================\n",
      "Audio loaded: 32.91s duration @ 16000 Hz\n",
      "Audio shape: (526629,)\n",
      "Input 'input_values' shape: torch.Size([1, 526629])\n",
      "Input 'attention_mask' shape: torch.Size([1, 526629])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 95.95%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      4.05% ██\n",
      "  Fake/Deepfake:  95.95% ███████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -1.5661\n",
      "  Fake:    1.5987\n",
      "======================================================================\n",
      "Processing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "======================================================================\n",
      "Audio loaded: 8.86s duration @ 16000 Hz\n",
      "Audio shape: (141689,)\n",
      "Input 'input_values' shape: torch.Size([1, 141689])\n",
      "Input 'attention_mask' shape: torch.Size([1, 141689])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: REAL/HUMAN\n",
      "📊 Confidence: 94.43%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:     94.43% ███████████████████████████████████████████████\n",
      "  Fake/Deepfake:   5.57% ██\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:    1.3936\n",
      "  Fake:   -1.4371\n",
      "======================================================================\n",
      "Processing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "======================================================================\n",
      "Audio loaded: 8.67s duration @ 16000 Hz\n",
      "Audio shape: (138763,)\n",
      "Input 'input_values' shape: torch.Size([1, 138763])\n",
      "Input 'attention_mask' shape: torch.Size([1, 138763])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 99.55%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      0.45% \n",
      "  Fake/Deepfake:  99.55% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -2.6714\n",
      "  Fake:    2.7355\n",
      "======================================================================\n",
      "Processing: clova.mp3\n",
      "======================================================================\n",
      "Audio loaded: 14.08s duration @ 16000 Hz\n",
      "Audio shape: (225251,)\n",
      "Input 'input_values' shape: torch.Size([1, 225251])\n",
      "Input 'attention_mask' shape: torch.Size([1, 225251])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 82.19%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:     17.81% ████████\n",
      "  Fake/Deepfake:  82.19% █████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -0.7637\n",
      "  Fake:    0.7657\n",
      "======================================================================\n",
      "Processing: sample voice 1.mp3\n",
      "======================================================================\n",
      "Audio loaded: 23.59s duration @ 16000 Hz\n",
      "Audio shape: (377418,)\n",
      "Input 'input_values' shape: torch.Size([1, 377418])\n",
      "Input 'attention_mask' shape: torch.Size([1, 377418])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 98.46%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      1.54% \n",
      "  Fake/Deepfake:  98.46% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -2.0568\n",
      "  Fake:    2.1035\n",
      "======================================================================\n",
      "Processing: voice_preview_mukundan - formal and clear.mp3\n",
      "======================================================================\n",
      "Audio loaded: 6.09s duration @ 16000 Hz\n",
      "Audio shape: (97385,)\n",
      "Input 'input_values' shape: torch.Size([1, 97385])\n",
      "Input 'attention_mask' shape: torch.Size([1, 97385])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 99.38%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      0.62% \n",
      "  Fake/Deepfake:  99.38% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -2.5093\n",
      "  Fake:    2.5744\n",
      "======================================================================\n",
      "Processing: voice_preview_martin li - raspy, serious and deep.mp3\n",
      "======================================================================\n",
      "Audio loaded: 22.18s duration @ 16000 Hz\n",
      "Audio shape: (354848,)\n",
      "Input 'input_values' shape: torch.Size([1, 354848])\n",
      "Input 'attention_mask' shape: torch.Size([1, 354848])\n",
      "\n",
      "======================================================================\n",
      "🎯 Prediction: FAKE/DEEPFAKE\n",
      "📊 Confidence: 99.56%\n",
      "======================================================================\n",
      "\n",
      "📈 Probabilities:\n",
      "  Real/Human:      0.44% \n",
      "  Fake/Deepfake:  99.56% █████████████████████████████████████████████████\n",
      "\n",
      "🔍 Raw Logits:\n",
      "  Real:   -2.6754\n",
      "  Fake:    2.7419\n",
      "\n",
      "\n",
      "======================================================================\n",
      "📊 SUMMARY OF ALL PREDICTIONS\n",
      "======================================================================\n",
      "Filename                                           Prediction      Confidence   Real%      Fake%\n",
      "----------------------------------------------------------------------\n",
      "medieval-gamer-voice-darkness-hunts-us-what-you... REAL/HUMAN       86.10%       86.1%      13.9%\n",
      "medieval-gamer-voice-you-can-view-our-website-a... FAKE/DEEPFAKE    99.24%        0.8%      99.2%\n",
      "narration_20251210_232729.mp3                      FAKE/DEEPFAKE    95.95%        4.1%      95.9%\n",
      "voice_preview_kanika - soft, smooth and muffled... REAL/HUMAN       94.43%       94.4%       5.6%\n",
      "voice_preview_faiq - standard, clear and neutra... FAKE/DEEPFAKE    99.55%        0.4%      99.6%\n",
      "clova.mp3                                          FAKE/DEEPFAKE    82.19%       17.8%      82.2%\n",
      "sample voice 1.mp3                                 FAKE/DEEPFAKE    98.46%        1.5%      98.5%\n",
      "voice_preview_mukundan - formal and clear.mp3      FAKE/DEEPFAKE    99.38%        0.6%      99.4%\n",
      "voice_preview_martin li - raspy, serious and de... FAKE/DEEPFAKE    99.56%        0.4%      99.6%\n",
      "\n",
      "======================================================================\n",
      "📈 DETECTION STATISTICS\n",
      "======================================================================\n",
      "Total files tested: 9\n",
      "Detected as REAL: 2 (22.2%)\n",
      "Detected as FAKE: 7 (77.8%)\n",
      "\n",
      "Average confidence: 94.99%\n",
      "\n",
      "✅ Files detected as REAL/HUMAN:\n",
      "  - medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3 (86.1% confidence)\n",
      "  - voice_preview_kanika - soft, smooth and muffled.mp3 (94.4% confidence)\n",
      "\n",
      "❌ Files detected as FAKE/DEEPFAKE:\n",
      "  - medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3 (99.2% confidence)\n",
      "  - narration_20251210_232729.mp3 (95.9% confidence)\n",
      "  - voice_preview_faiq - standard, clear and neutral.mp3 (99.6% confidence)\n",
      "  - clova.mp3 (82.2% confidence)\n",
      "  - sample voice 1.mp3 (98.5% confidence)\n",
      "  - voice_preview_mukundan - formal and clear.mp3 (99.4% confidence)\n",
      "  - voice_preview_martin li - raspy, serious and deep.mp3 (99.6% confidence)\n",
      "\n",
      "📋 DETAILED BREAKDOWN:\n",
      "Filename                                      Real%      Fake%      Verdict\n",
      "----------------------------------------------------------------------\n",
      "medieval-gamer-voice-darkness-hunts-us-wha...  86.10%     13.90%    ✓ REAL\n",
      "medieval-gamer-voice-you-can-view-our-webs...   0.76%     99.24%    ✗ FAKE\n",
      "narration_20251210_232729.mp3                   4.05%     95.95%    ✗ FAKE\n",
      "voice_preview_kanika - soft, smooth and mu...  94.43%      5.57%    ✓ REAL\n",
      "voice_preview_faiq - standard, clear and n...   0.45%     99.55%    ✗ FAKE\n",
      "clova.mp3                                      17.81%     82.19%    ✗ FAKE\n",
      "sample voice 1.mp3                              1.54%     98.46%    ✗ FAKE\n",
      "voice_preview_mukundan - formal and clear.mp3   0.62%     99.38%    ✗ FAKE\n",
      "voice_preview_martin li - raspy, serious a...   0.44%     99.56%    ✗ FAKE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "model_path = r\"wav2vec2-deepfake-voice-detector\"  # Update to local path if cloned\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Load model and feature extractor\n",
    "print(\"Loading garystafford/wav2vec2-deepfake-voice-detector model...\")\n",
    "try:\n",
    "    # Try loading from local path first\n",
    "    if os.path.exists(model_path):\n",
    "        model = AutoModelForAudioClassification.from_pretrained(model_path, local_files_only=True)\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_path, local_files_only=True)\n",
    "        print(f\"✅ Model loaded from local path: {model_path}\")\n",
    "    else:\n",
    "        # Load from HuggingFace\n",
    "        model_name = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "        model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        print(\"✅ Model loaded from HuggingFace\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from local, trying HuggingFace: {e}\")\n",
    "    model_name = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    print(\"✅ Model loaded from HuggingFace\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model on {device}\\n\")\n",
    "\n",
    "# Check model configuration\n",
    "print(\"Model Configuration:\")\n",
    "if hasattr(model.config, 'id2label'):\n",
    "    print(f\"Labels: {model.config.id2label}\")\n",
    "else:\n",
    "    print(\"Labels: {0: 'real', 1: 'fake'} (default)\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\\n\")\n",
    "\n",
    "def predict_audio(audio_path, show_details=True):\n",
    "    \"\"\"Predict if audio is real or fake/deepfake\"\"\"\n",
    "    try:\n",
    "        if show_details:\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "            print('='*70)\n",
    "        \n",
    "        # Load and preprocess audio (automatically resamples to 16kHz)\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sample_rate, mono=True)\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"Audio loaded: {len(audio)/sr:.2f}s duration @ {sr} Hz\")\n",
    "            print(f\"Audio shape: {audio.shape}\")\n",
    "        \n",
    "        # Process with feature extractor\n",
    "        inputs = feature_extractor(\n",
    "            audio, \n",
    "            sampling_rate=target_sample_rate, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        if show_details:\n",
    "            for key, val in inputs.items():\n",
    "                print(f\"Input '{key}' shape: {val.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get predictions\n",
    "        # Class 0: Real, Class 1: Fake\n",
    "        prob_real = probs[0][0].item()\n",
    "        prob_fake = probs[0][1].item()\n",
    "        \n",
    "        # Determine prediction\n",
    "        if prob_fake > 0.5:\n",
    "            prediction = \"FAKE/DEEPFAKE\"\n",
    "            confidence = prob_fake\n",
    "        else:\n",
    "            prediction = \"REAL/HUMAN\"\n",
    "            confidence = prob_real\n",
    "        \n",
    "        # Display results\n",
    "        if show_details:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"🎯 Prediction: {prediction}\")\n",
    "            print(f\"📊 Confidence: {confidence*100:.2f}%\")\n",
    "            print('='*70)\n",
    "            \n",
    "            print(f\"\\n📈 Probabilities:\")\n",
    "            print(f\"  Real/Human:    {prob_real*100:6.2f}% {'█' * int(prob_real * 50)}\")\n",
    "            print(f\"  Fake/Deepfake: {prob_fake*100:6.2f}% {'█' * int(prob_fake * 50)}\")\n",
    "            \n",
    "            print(f\"\\n🔍 Raw Logits:\")\n",
    "            print(f\"  Real:  {logits[0][0].item():8.4f}\")\n",
    "            print(f\"  Fake:  {logits[0][1].item():8.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": os.path.basename(audio_path),\n",
    "            \"prediction\": prediction,\n",
    "            \"confidence\": confidence,\n",
    "            \"prob_real\": prob_real,\n",
    "            \"prob_fake\": prob_fake,\n",
    "            \"logits\": logits.cpu().numpy()[0]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {os.path.basename(audio_path)}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test multiple files\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"BATCH TESTING - Multiple Files\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_files = [\n",
    "    r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "    r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\",\n",
    "    r\"narration_20251210_232729.mp3\",\n",
    "    r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "    r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "    r\"clova.mp3\",\n",
    "    r\"sample voice 1.mp3\",\n",
    "    r\"voice_preview_mukundan - formal and clear.mp3\",\n",
    "    r\"voice_preview_martin li - raspy, serious and deep.mp3\"\n",
    "    # Add more files here\n",
    "]\n",
    "\n",
    "results = []\n",
    "for audio_file in test_files:\n",
    "    if os.path.exists(audio_file):\n",
    "        result = predict_audio(audio_file, show_details=True)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️  File not found: {audio_file}\")\n",
    "\n",
    "\n",
    "# Summary Report\n",
    "if results:\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"📊 SUMMARY OF ALL PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Filename':<50} {'Prediction':<15} {'Confidence':<12} {'Real%':<10} {'Fake%'}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for r in results:\n",
    "        filename = r['filename'][:47] + \"...\" if len(r['filename']) > 50 else r['filename']\n",
    "        pred_short = r['prediction'][:12] + \"...\" if len(r['prediction']) > 15 else r['prediction']\n",
    "        print(f\"{filename:<50} {pred_short:<15} {r['confidence']*100:>6.2f}%      {r['prob_real']*100:>5.1f}%     {r['prob_fake']*100:>5.1f}%\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📈 DETECTION STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    real_count = sum(1 for r in results if \"REAL\" in r['prediction'])\n",
    "    fake_count = sum(1 for r in results if \"FAKE\" in r['prediction'])\n",
    "    \n",
    "    print(f\"Total files tested: {len(results)}\")\n",
    "    print(f\"Detected as REAL: {real_count} ({real_count/len(results)*100:.1f}%)\")\n",
    "    print(f\"Detected as FAKE: {fake_count} ({fake_count/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    avg_confidence = np.mean([r['confidence'] for r in results])\n",
    "    print(f\"\\nAverage confidence: {avg_confidence*100:.2f}%\")\n",
    "    \n",
    "    # List by category\n",
    "    print(f\"\\n✅ Files detected as REAL/HUMAN:\")\n",
    "    for r in results:\n",
    "        if \"REAL\" in r['prediction']:\n",
    "            print(f\"  - {r['filename']} ({r['confidence']*100:.1f}% confidence)\")\n",
    "    \n",
    "    print(f\"\\n❌ Files detected as FAKE/DEEPFAKE:\")\n",
    "    for r in results:\n",
    "        if \"FAKE\" in r['prediction']:\n",
    "            print(f\"  - {r['filename']} ({r['confidence']*100:.1f}% confidence)\")\n",
    "    \n",
    "    # Show detailed breakdown\n",
    "    print(f\"\\n📋 DETAILED BREAKDOWN:\")\n",
    "    print(f\"{'Filename':<45} {'Real%':<10} {'Fake%':<10} {'Verdict'}\")\n",
    "    print(\"-\"*70)\n",
    "    for r in results:\n",
    "        filename = r['filename'][:42] + \"...\" if len(r['filename']) > 45 else r['filename']\n",
    "        verdict = \"✓ REAL\" if \"REAL\" in r['prediction'] else \"✗ FAKE\"\n",
    "        print(f\"{filename:<45} {r['prob_real']*100:>6.2f}%    {r['prob_fake']*100:>6.2f}%    {verdict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397e0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: 40% Physics, 60% Deep Learning\n",
      "======================================================================\n",
      "🔧 Initializing Hybrid Detector\n",
      "   Device: cpu\n",
      "   Physics Weight: 40%\n",
      "   DL Weight: 60%\n",
      "📥 Loading model from 'wav2vec2-deepfake-voice-detector'...\n",
      "✅ Deep Learning Model Loaded Successfully\n",
      "✅ Hybrid Detector Ready\n",
      "\n",
      "\n",
      "📊 Result:\n",
      "   Status: success\n",
      "   Classification: AI_GENERATED\n",
      "   Confidence: 0.9399999976158142\n",
      "   Explanation: Deep learning model detected synthetic voice patterns (confidence: 98.5%); Robotic pitch modulation patterns (CV: 0.20, expected: >0.22)\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.8610000014305115\n",
      "   dl_score: 0.985\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   physics_features: {'pitch_cv': np.float64(0.19643575289740356), 'intensity_std': np.float32(0.07224538), 'freq_skew': np.float64(1.2587673386321117), 'mean_pitch': np.float64(121.40441626562153), 'std_pitch': np.float64(23.848167914207153)}\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Base64 Input\n",
      "======================================================================\n",
      "\n",
      "📊 Result:\n",
      "   Classification: AI_GENERATED\n",
      "   Confidence: 0.9399999976158142\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Dynamic Weight Adjustment (80-20)\n",
      "======================================================================\n",
      "⚙️  Weights updated:\n",
      "   Physics: 20%\n",
      "   DL: 80%\n",
      "\n",
      "📊 Result with new weights:\n",
      "   Classification: AI_GENERATED\n",
      "   Confidence: 0.9599999785423279\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: Batch Processing Multiple Files\n",
      "======================================================================\n",
      "\n",
      "Filename                                 Classification  Confidence   Method\n",
      "----------------------------------------------------------------------\n",
      "medieval-gamer-voice-darkness-hunts-u... HUMAN           0.29        \n",
      "medieval-gamer-voice-you-can-view-our... AI_GENERATED    0.87        \n",
      "narration_20251210_232729.mp3...         AI_GENERATED    0.88        \n",
      "voice_preview_kanika - soft, smooth a... HUMAN           0.23        \n",
      "voice_preview_faiq - standard, clear ... AI_GENERATED    0.97        \n",
      "clova.mp3...                             AI_GENERATED    0.83        \n",
      "sample voice 1.mp3...                    AI_GENERATED    0.96        \n",
      "voice_preview_mukundan - formal and c... AI_GENERATED    0.96        \n",
      "voice_preview_martin li - raspy, seri... AI_GENERATED    0.91        \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    \"\"\"\n",
    "    Hybrid AI Voice Detection System combining:\n",
    "    1. Physics-based acoustic analysis\n",
    "    2. Deep Learning model (garystafford/wav2vec2-deepfake-voice-detector)\n",
    "    \n",
    "    Configurable ensemble weights for flexibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path=\"wav2vec2-deepfake-voice-detector\",\n",
    "        physics_weight=0.5,\n",
    "        dl_weight=0.5,\n",
    "        use_local_model=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid detector\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to model (local or HuggingFace)\n",
    "            physics_weight: Weight for physics score (0-1)\n",
    "            dl_weight: Weight for DL score (0-1)\n",
    "            use_local_model: Whether to load from local path\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total_weight\n",
    "        self.dl_weight = dl_weight / total_weight\n",
    "        \n",
    "        print(f\"🔧 Initializing Hybrid Detector\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Physics Weight: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL Weight: {self.dl_weight*100:.0f}%\")\n",
    "        \n",
    "        # --- LOAD DEEP LEARNING MODEL ---\n",
    "        try:\n",
    "            print(f\"📥 Loading model from '{model_path}'...\")\n",
    "            \n",
    "            if use_local_model:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(\n",
    "                    model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "                    model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "            else:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(model_path)\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "            \n",
    "            self.dl_model.to(self.device)\n",
    "            self.dl_model.eval()\n",
    "            self.dl_ready = True\n",
    "            print(\"✅ Deep Learning Model Loaded Successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  DL Model Load Failed: {e}\")\n",
    "            print(\"   Running in Physics-Only mode\")\n",
    "            self.dl_ready = False\n",
    "            self.dl_weight = 0\n",
    "            self.physics_weight = 1.0\n",
    "\n",
    "        # --- PHYSICS ENGINE PARAMETERS ---\n",
    "        # Tuned thresholds for modern TTS detection\n",
    "        self.CV_AI_THRESHOLD = 0.20      # Coefficient of variation threshold for AI\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32   # CV threshold for human\n",
    "        self.INTENSITY_MIN_STD = 0.05    # Minimum intensity std for human\n",
    "        self.INTENSITY_MAX_STD = 0.15    # Maximum intensity std\n",
    "        \n",
    "        print(\"✅ Hybrid Detector Ready\\n\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # HELPER: Base64 Decoding\n",
    "    # ==========================================================\n",
    "    def decode_base64_audio(self, base64_string):\n",
    "        \"\"\"\n",
    "        Decode base64 audio and save to temporary file\n",
    "        \n",
    "        Args:\n",
    "            base64_string: Base64 encoded audio data\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to temporary audio file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode base64\n",
    "            audio_data = base64.b64decode(base64_string)\n",
    "            \n",
    "            # Create temporary file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
    "            temp_file.write(audio_data)\n",
    "            temp_file.close()\n",
    "            \n",
    "            return temp_file.name\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to decode base64 audio: {str(e)}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: PHYSICS ENGINE\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\"Linear interpolation for scoring\"\"\"\n",
    "        if val <= min_val:\n",
    "            return 1.0\n",
    "        if val >= max_val:\n",
    "            return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using physics-based acoustic features\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, method, features_dict)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio at native sample rate\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Robust pitch tracking using PYIN\n",
    "            f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "                y, \n",
    "                fmin=50, \n",
    "                fmax=400, \n",
    "                sr=sr\n",
    "            )\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0:\n",
    "                return 0.0, \"No voice detected\", {}\n",
    "\n",
    "            # Extract acoustic features\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            # Calculate feature metrics\n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch if mean_pitch > 0 else 0,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid),\n",
    "                'mean_pitch': mean_pitch,\n",
    "                'std_pitch': std_pitch\n",
    "            }\n",
    "\n",
    "            # Individual feature scores (higher = more AI-like)\n",
    "            intensity_score = self.get_linear_score(\n",
    "                feats['intensity_std'], \n",
    "                self.INTENSITY_MIN_STD, \n",
    "                self.INTENSITY_MAX_STD\n",
    "            )\n",
    "            \n",
    "            pitch_score = self.get_linear_score(\n",
    "                feats['pitch_cv'], \n",
    "                self.CV_AI_THRESHOLD, \n",
    "                self.CV_HUMAN_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            skew_score = self.get_linear_score(\n",
    "                abs(feats['freq_skew']), \n",
    "                0.1, \n",
    "                1.0\n",
    "            )\n",
    "\n",
    "            # Weighted combination\n",
    "            W_INTENSITY = 0.40\n",
    "            W_PITCH = 0.40\n",
    "            W_SKEW = 0.20\n",
    "            \n",
    "            base_score = (\n",
    "                intensity_score * W_INTENSITY + \n",
    "                pitch_score * W_PITCH + \n",
    "                skew_score * W_SKEW\n",
    "            )\n",
    "\n",
    "            # Synergy bonus: if both intensity and pitch are suspicious\n",
    "            if intensity_score > 0.4 and pitch_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: DEEP LEARNING ENGINE\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using deep learning model\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, label)\n",
    "        \"\"\"\n",
    "        if not self.dl_ready:\n",
    "            return 0.0, \"Model not loaded\"\n",
    "\n",
    "        target_sr = 16000\n",
    "\n",
    "        try:\n",
    "            # Load audio with librosa\n",
    "            waveform_np, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "\n",
    "            # Process with feature extractor\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform_np,\n",
    "                sampling_rate=target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.dl_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            # Get predictions\n",
    "            # Class 0: Real, Class 1: Fake\n",
    "            prob_real = probs[0][0].item()\n",
    "            prob_fake = probs[0][1].item()\n",
    "            \n",
    "            # AI score is the fake probability\n",
    "            ai_score = prob_fake\n",
    "            \n",
    "            label = \"Fake/Deepfake\" if prob_fake > 0.5 else \"Real/Human\"\n",
    "\n",
    "            return round(ai_score, 3), label\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: EXPLANATION GENERATOR\n",
    "    # ==========================================================\n",
    "    def generate_explanation(self, final_score, phys_score, dl_score, dl_label, phys_feats):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the classification\n",
    "        \n",
    "        Returns:\n",
    "            str: Explanation text\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        if final_score > 0.55:\n",
    "            # AI GENERATED\n",
    "            \n",
    "            # Deep Learning contributions\n",
    "            if dl_score > 0.55 and self.dl_ready:\n",
    "                if \"Fake\" in dl_label or \"Deepfake\" in dl_label:\n",
    "                    explanations.append(\n",
    "                        f\"Deep learning model detected synthetic voice patterns \"\n",
    "                        f\"(confidence: {dl_score*100:.1f}%)\"\n",
    "                    )\n",
    "            \n",
    "            # Physics contributions\n",
    "            if phys_score > 0.55:\n",
    "                p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                i_std = phys_feats.get('intensity_std', 0)\n",
    "                \n",
    "                if i_std < 0.06:\n",
    "                    explanations.append(\n",
    "                        f\"Unnaturally consistent energy levels detected \"\n",
    "                        f\"(std: {i_std:.3f}, expected: >0.06)\"\n",
    "                    )\n",
    "                \n",
    "                if p_cv < 0.22:\n",
    "                    explanations.append(\n",
    "                        f\"Robotic pitch modulation patterns \"\n",
    "                        f\"(CV: {p_cv:.2f}, expected: >0.22)\"\n",
    "                    )\n",
    "                \n",
    "                if not explanations or (i_std >= 0.06 and p_cv >= 0.22):\n",
    "                    explanations.append(\n",
    "                        \"Acoustic parameters lack natural human variability\"\n",
    "                    )\n",
    "            \n",
    "            if not explanations:\n",
    "                explanations.append(\n",
    "                    \"Voice exhibits characteristics consistent with AI generation\"\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            # HUMAN\n",
    "            explanations.append(\n",
    "                \"Voice exhibits natural acoustic variability and human speech characteristics\"\n",
    "            )\n",
    "        \n",
    "        return \"; \".join(explanations)\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART D: MAIN ANALYSIS FUNCTION\n",
    "    # ==========================================================\n",
    "    def analyze(self, audio_input, input_type=\"file\"):\n",
    "        \"\"\"\n",
    "        Main analysis function with configurable input types\n",
    "        \n",
    "        Args:\n",
    "            audio_input: Either file path or base64 string\n",
    "            input_type: \"file\" or \"base64\"\n",
    "            \n",
    "        Returns:\n",
    "            dict: Analysis results following API response format\n",
    "        \"\"\"\n",
    "        temp_file = None\n",
    "        \n",
    "        try:\n",
    "            # Handle input type\n",
    "            if input_type == \"base64\":\n",
    "                temp_file = self.decode_base64_audio(audio_input)\n",
    "                audio_path = temp_file\n",
    "            elif input_type == \"file\":\n",
    "                audio_path = audio_input\n",
    "                if not os.path.exists(audio_path):\n",
    "                    return {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": f\"Audio file not found: {audio_path}\"\n",
    "                    }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"Invalid input_type: {input_type}. Use 'file' or 'base64'\"\n",
    "                }\n",
    "\n",
    "            # 1. Run Physics Analysis\n",
    "            phys_score, phys_method, phys_feats = self.get_physics_score(audio_path)\n",
    "            \n",
    "            # 2. Run Deep Learning Analysis\n",
    "            dl_score, dl_label = self.get_dl_score(audio_path)\n",
    "\n",
    "            # 3. Calculate weighted ensemble score\n",
    "            final_score = (\n",
    "                self.physics_weight * phys_score + \n",
    "                self.dl_weight * dl_score\n",
    "            )\n",
    "            \n",
    "            # Round to 2 decimal places\n",
    "            final_score = round(final_score, 2)\n",
    "            \n",
    "            # 4. Determine classification\n",
    "            classification = \"AI_GENERATED\" if final_score > 0.55 else \"HUMAN\"\n",
    "            \n",
    "            # 5. Generate explanation\n",
    "            explanation = self.generate_explanation(\n",
    "                final_score, \n",
    "                phys_score, \n",
    "                dl_score, \n",
    "                dl_label, \n",
    "                phys_feats\n",
    "            )\n",
    "\n",
    "            # 6. Return API-compliant response\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": final_score,\n",
    "                \"explanation\": explanation,\n",
    "                \"debug\": {\n",
    "                    \"physics_score\": phys_score,\n",
    "                    \"dl_score\": dl_score,\n",
    "                    \"dl_label\": dl_label,\n",
    "                    \"physics_weight\": f\"{self.physics_weight*100:.0f}%\",\n",
    "                    \"dl_weight\": f\"{self.dl_weight*100:.0f}%\",\n",
    "                    \"physics_features\": phys_feats\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            if temp_file and os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.unlink(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # ==========================================================\n",
    "    # UTILITY: Update Weights\n",
    "    # ==========================================================\n",
    "    def update_weights(self, physics_weight, dl_weight):\n",
    "        \"\"\"\n",
    "        Update ensemble weights dynamically\n",
    "        \n",
    "        Args:\n",
    "            physics_weight: New physics weight (0-1)\n",
    "            dl_weight: New DL weight (0-1)\n",
    "        \"\"\"\n",
    "        total = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total\n",
    "        self.dl_weight = dl_weight / total\n",
    "        \n",
    "        print(f\"⚙️  Weights updated:\")\n",
    "        print(f\"   Physics: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL: {self.dl_weight*100:.0f}%\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# USAGE EXAMPLES\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Initialize with 60-40 split (Physics-DL)\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXAMPLE 1: 40% Physics, 60% Deep Learning\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    detector = HybridEnsembleDetector(\n",
    "        model_path=\"wav2vec2-deepfake-voice-detector\",\n",
    "        physics_weight=0.4,\n",
    "        dl_weight=0.6,\n",
    "        use_local_model=True  # Set True if using local model\n",
    "    )\n",
    "    \n",
    "    # Test with file path\n",
    "    audio_file = r\"sample voice 1.mp3\"\n",
    "    result = detector.analyze(audio_file, input_type=\"file\")\n",
    "    \n",
    "    print(f\"\\n📊 Result:\")\n",
    "    print(f\"   Status: {result['status']}\")\n",
    "    print(f\"   Classification: {result['classification']}\")\n",
    "    print(f\"   Confidence: {result['confidenceScore']}\")\n",
    "    print(f\"   Explanation: {result['explanation']}\")\n",
    "    print(f\"\\n🔍 Debug Info:\")\n",
    "    for key, val in result.get('debug', {}).items():\n",
    "        print(f\"   {key}: {val}\")\n",
    "    \n",
    "    \n",
    "    # Example 2: Test with base64 input\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Base64 Input\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Read file and convert to base64\n",
    "    with open(audio_file, 'rb') as f:\n",
    "        audio_bytes = f.read()\n",
    "        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "    \n",
    "    result_b64 = detector.analyze(audio_base64, input_type=\"base64\")\n",
    "    \n",
    "    print(f\"\\n📊 Result:\")\n",
    "    print(f\"   Classification: {result_b64['classification']}\")\n",
    "    print(f\"   Confidence: {result_b64['confidenceScore']}\")\n",
    "    \n",
    "    \n",
    "    # Example 3: Change weights dynamically\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Dynamic Weight Adjustment (80-20)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    detector.update_weights(physics_weight=0.2, dl_weight=0.8)\n",
    "    \n",
    "    result_new = detector.analyze(audio_file, input_type=\"file\")\n",
    "    \n",
    "    print(f\"\\n📊 Result with new weights:\")\n",
    "    print(f\"   Classification: {result_new['classification']}\")\n",
    "    print(f\"   Confidence: {result_new['confidenceScore']}\")\n",
    "    \n",
    "    \n",
    "    # Example 4: Batch processing\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Batch Processing Multiple Files\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_files = [\n",
    "        r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "        r\"medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\",\n",
    "        r\"narration_20251210_232729.mp3\",\n",
    "        r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "        r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "        r\"clova.mp3\",\n",
    "        r\"sample voice 1.mp3\",\n",
    "        r\"voice_preview_mukundan - formal and clear.mp3\",\n",
    "        r\"voice_preview_martin li - raspy, serious and deep.mp3\"\n",
    "        # Add more files here\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Filename':<40} {'Classification':<15} {'Confidence':<12} {'Method'}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            result = detector.analyze(file_path, input_type=\"file\")\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                filename = os.path.basename(file_path)[:37] + \"...\"\n",
    "                print(f\"{filename:<40} {result['classification']:<15} {result['confidenceScore']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b21eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing Hybrid Detector on cpu\n",
      "📥 Loading Deepfake Model...\n",
      "✅ Deepfake Model Loaded.\n",
      "📥 Loading Whisper-Tiny for LID...\n",
      "✅ Language Model Loaded.\n",
      "✅ System Ready\n",
      "\n",
      "{'status': 'success', 'language': 'Unknown', 'classification': 'AI_GENERATED', 'confidenceScore': 0.95, 'explanation': 'Unnaturally consistent energy detected (std: 0.010); Robotic pitch modulation detected (CV: 0.08)', 'debug': {'phys_score': 0.95, 'dl_score': 0.0, 'raw_lang': 'Unknown'}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import base64\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    \"\"\"\n",
    "    Hybrid AI Voice Detection System (Production Version)\n",
    "    1. Physics: Signal Processing (Librosa)\n",
    "    2. Deep Learning: Wav2Vec2\n",
    "    3. Language ID: Whisper-Tiny (Logit-Based, No Translation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        deepfake_model_path=\"wav2vec2-deepfake-voice-detector\",\n",
    "        use_local_model=False\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"🔧 Initializing Hybrid Detector on {self.device}\")\n",
    "\n",
    "        # --- 1. LOAD DEEPFAKE DETECTION MODEL ---\n",
    "        print(f\"📥 Loading Deepfake Model...\")\n",
    "        try:\n",
    "            self.df_model = AutoModelForAudioClassification.from_pretrained(\n",
    "                deepfake_model_path, local_files_only=use_local_model\n",
    "            ).to(self.device)\n",
    "            self.df_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "                deepfake_model_path, local_files_only=use_local_model\n",
    "            )\n",
    "            self.df_model.eval()\n",
    "            self.df_ready = True\n",
    "            print(\"✅ Deepfake Model Loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Deepfake Model Failed: {e}\")\n",
    "            self.df_ready = False\n",
    "\n",
    "        # --- 2. LOAD WHISPER LANGUAGE ID (Logit Mode) ---\n",
    "        print(\"📥 Loading Whisper-Tiny for LID...\")\n",
    "        try:\n",
    "            self.lid_processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "            self.lid_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").to(self.device)\n",
    "            self.lid_model.eval()\n",
    "            self.lid_ready = True\n",
    "            \n",
    "            # Prepare Language Token Map (id -> language name)\n",
    "            # Whisper stores languages as special tokens (e.g., <|hi|>, <|ta|>)\n",
    "            # We extract just the language tokens to check their probabilities\n",
    "            self.lang_code_to_id = {\n",
    "                code: id for code, id in self.lid_processor.tokenizer.get_vocab().items() \n",
    "                if code.startswith(\"<|\") and code.endswith(\"|>\") and len(code) == 4\n",
    "            }\n",
    "            # Custom map for your requirements\n",
    "            self.code_to_name = {\n",
    "                \"<|en|>\": \"English\", \"<|hi|>\": \"Hindi\", \"<|ta|>\": \"Tamil\",\n",
    "                \"<|te|>\": \"Telugu\", \"<|ml|>\": \"Malayalam\", \"<|kn|>\": \"Kannada\",\n",
    "                \"<|mr|>\": \"Marathi\", \"<|bn|>\": \"Bengali\", \"<|ur|>\": \"Urdu\"\n",
    "            }\n",
    "            print(\"✅ Language Model Loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Language Model Failed: {e}\")\n",
    "            self.lid_ready = False\n",
    "\n",
    "        # --- 3. PHYSICS PARAMETERS ---\n",
    "        self.CV_AI_THRESHOLD = 0.20\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32\n",
    "        self.INTENSITY_MIN_STD = 0.05\n",
    "        self.INTENSITY_MAX_STD = 0.15 \n",
    "        \n",
    "        print(\"✅ System Ready\\n\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: LANGUAGE DETECTION (Logit-Based / No Translation)\n",
    "    # ==========================================================\n",
    "    def detect_language(self, audio_path):\n",
    "        if not self.lid_ready: return \"Unknown\"\n",
    "\n",
    "        try:\n",
    "            # 1. Load Audio (16kHz)\n",
    "            audio, _ = librosa.load(audio_path, sr=16000)\n",
    "            \n",
    "            # 2. Prepare Inputs\n",
    "            input_features = self.lid_processor(\n",
    "                audio, sampling_rate=16000, return_tensors=\"pt\"\n",
    "            ).input_features.to(self.device)\n",
    "\n",
    "            # 3. Create Decoder Start Token\n",
    "            # We only want to predict the FIRST token after start\n",
    "            decoder_input_ids = torch.tensor([[self.lid_model.config.decoder_start_token_id]]).to(self.device)\n",
    "\n",
    "            # 4. Forward Pass (No Generation Loop)\n",
    "            with torch.no_grad():\n",
    "                logits = self.lid_model(input_features, decoder_input_ids=decoder_input_ids).logits\n",
    "            \n",
    "            # 5. Extract Logits for First Token: (Batch, Seq, Vocab) -> (Vocab)\n",
    "            first_token_logits = logits[0, 0, :]\n",
    "\n",
    "            # 6. Find the Language Token with Max Probability\n",
    "            best_lang_code = \"Unknown\"\n",
    "            max_score = -float('inf')\n",
    "\n",
    "            for code, token_id in self.lang_code_to_id.items():\n",
    "                score = first_token_logits[token_id].item()\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_lang_code = code\n",
    "\n",
    "            # 7. Map to Readable Name\n",
    "            return self.code_to_name.get(best_lang_code, best_lang_code.replace(\"<|\", \"\").replace(\"|>\", \"\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Lang Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: PHYSICS ENGINE\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        if val <= min_val: return 1.0\n",
    "        if val >= max_val: return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            f0, _, _ = librosa.pyin(y, fmin=50, fmax=400, sr=sr)\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0: return 0.0, \"No voice detected\", {}\n",
    "\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch if mean_pitch > 0 else 0,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid)\n",
    "            }\n",
    "\n",
    "            i_score = self.get_linear_score(feats['intensity_std'], self.INTENSITY_MIN_STD, self.INTENSITY_MAX_STD)\n",
    "            p_score = self.get_linear_score(feats['pitch_cv'], self.CV_AI_THRESHOLD, self.CV_HUMAN_THRESHOLD)\n",
    "            s_score = self.get_linear_score(abs(feats['freq_skew']), 0.1, 1.0)\n",
    "\n",
    "            # Weighted Physics Score\n",
    "            base_score = (i_score * 0.40) + (p_score * 0.40) + (s_score * 0.20)\n",
    "\n",
    "            # Synergy Bonus\n",
    "            if i_score > 0.4 and p_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: DEEP LEARNING ENGINE\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        if not self.df_ready: return 0.0, \"Model not loaded\"\n",
    "\n",
    "        try:\n",
    "            waveform, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "            inputs = self.df_extractor(\n",
    "                waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.df_model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            prob_fake = probs[0][1].item() \n",
    "            label = \"Fake\" if prob_fake > 0.5 else \"Real\"\n",
    "\n",
    "            return round(prob_fake, 3), label\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART D: MAIN ANALYZE FUNCTION (VETO LOGIC APPLIED)\n",
    "    # ==========================================================\n",
    "    def analyze(self, audio_input, input_type=\"file\"):\n",
    "        temp_file = None\n",
    "        try:\n",
    "            if input_type == \"base64\":\n",
    "                audio_data = base64.b64decode(audio_input)\n",
    "                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
    "                temp_file.write(audio_data)\n",
    "                temp_file.close()\n",
    "                audio_path = temp_file.name\n",
    "            else:\n",
    "                audio_path = audio_input\n",
    "\n",
    "            if not os.path.exists(audio_path):\n",
    "                return {\"status\": \"error\", \"error\": \"File not found\"}\n",
    "\n",
    "            # 1. Detect Language\n",
    "            detected_lang = self.detect_language(audio_path)\n",
    "\n",
    "            # 2. Run Analysis Engines\n",
    "            phys_score, _, phys_feats = self.get_physics_score(audio_path)\n",
    "            dl_score, dl_label = self.get_dl_score(audio_path)\n",
    "\n",
    "            # 3. VETO LOGIC (The Fix for 0.94 vs 0.05)\n",
    "            # If Physics says \"DEFINITELY AI\" (>0.85), trust it even if DL misses.\n",
    "            # If DL says \"DEFINITELY AI\" (>0.85), trust it even if Physics misses.\n",
    "            if phys_score > 0.85 or dl_score > 0.85:\n",
    "                final_score = max(phys_score, dl_score)\n",
    "            else:\n",
    "                # Otherwise, take a weighted average favoring the higher signal\n",
    "                final_score = (0.6 * max(phys_score, dl_score)) + (0.4 * min(phys_score, dl_score))\n",
    "\n",
    "            final_score = round(final_score, 2)\n",
    "            \n",
    "            # 4. Classification\n",
    "            classification = \"AI_GENERATED\" if final_score > 0.55 else \"HUMAN\"\n",
    "\n",
    "            # 5. Explanations\n",
    "            explanations = []\n",
    "            if classification == \"AI_GENERATED\":\n",
    "                if dl_score > 0.55:\n",
    "                    explanations.append(f\"Deep learning model detected synthetic artifacts (conf: {dl_score})\")\n",
    "                if phys_score > 0.55:\n",
    "                    p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                    i_std = phys_feats.get('intensity_std', 0)\n",
    "                    if i_std < 0.06: explanations.append(f\"Unnaturally consistent energy detected (std: {i_std:.3f})\")\n",
    "                    if p_cv < 0.22: explanations.append(f\"Robotic pitch modulation detected (CV: {p_cv:.2f})\")\n",
    "                if not explanations: explanations.append(\"Acoustic fingerprint matches AI characteristics\")\n",
    "            else:\n",
    "                explanations.append(\"Voice exhibits natural acoustic variability\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"language\": detected_lang,\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": final_score,\n",
    "                \"explanation\": \"; \".join(explanations),\n",
    "                \"debug\": {\n",
    "                    \"phys_score\": phys_score,\n",
    "                    \"dl_score\": dl_score,\n",
    "                    \"raw_lang\": detected_lang\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "        finally:\n",
    "            if temp_file and os.path.exists(temp_file): os.unlink(temp_file)\n",
    "\n",
    "# --- USAGE ---\n",
    "if __name__ == \"__main__\":\n",
    "    detector = HybridEnsembleDetector(\n",
    "        deepfake_model_path=\"wav2vec2-deepfake-voice-detector\",\n",
    "        use_local_model=True\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    result = detector.analyze(r\"\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "952842a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If audio > 30 sec break it into first 30 sec then analyze. Add language detection using whisperAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b351732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Initialize Hybrid Detector with Language Detection\n",
      "======================================================================\n",
      "🔧 Initializing Hybrid Detector with Language Detection\n",
      "   Device: cpu\n",
      "   Physics Weight: 40%\n",
      "   DL Weight: 60%\n",
      "   Max Audio Duration: 30s\n",
      "📥 Loading deepfake detection model from 'wav2vec2-deepfake-voice-detector'...\n",
      "✅ Deepfake Detection Model Loaded\n",
      "📥 Loading Whisper model for language detection from 'openai/whisper-base'...\n",
      "✅ Whisper Language Detection Model Loaded\n",
      "✅ Hybrid Detector Ready\n",
      "\n",
      "🎵 Analyzing: sample voice 1.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "\n",
      "📊 Result:\n",
      "   Status: success\n",
      "   Language: English\n",
      "   Classification: AI_GENERATED\n",
      "   Confidence: 0.59\n",
      "   Explanation: Deep learning model detected synthetic voice patterns (confidence: 98.5%)\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.985\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Batch Processing with Language Detection\n",
      "======================================================================\n",
      "\n",
      "Filename                                 Language        Classification  Confidence\n",
      "--------------------------------------------------------------------------------\n",
      "🎵 Analyzing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "voice_preview_faiq - standard, clear ... Hindi           AI_GENERATED    0.60\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.996\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: clova.mp3\n",
      "   🌐 Language: Analyzing transcription...\n",
      "   🌐 Detected Language: English (default)\n",
      "clova.mp3...                             English         HUMAN           0.49\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.822\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: sample voice 1.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "sample voice 1.mp3...                    English         AI_GENERATED    0.59\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.985\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: voice_preview_mukundan - formal and clear.mp3\n",
      "   🌐 Detected Language: Tamil (ta)\n",
      "voice_preview_mukundan - formal and c... Tamil           AI_GENERATED    0.60\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.994\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "voice_preview_kanika - soft, smooth a... Hindi           HUMAN           0.03\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.056\n",
      "   dl_label: Real/Human\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "medieval-gamer-voice-darkness-hunts-u... English         HUMAN           0.08\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.139\n",
      "   dl_label: Real/Human\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n",
      "🎵 Analyzing: voice_preview_tarini - soft, cheerful and expressive.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "voice_preview_tarini - soft, cheerful... Hindi           AI_GENERATED    0.58\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.0\n",
      "   dl_score: 0.967\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 0\n",
      "   was_truncated: False\n",
      "   physics_features: {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    \"\"\"\n",
    "    Hybrid AI Voice Detection System with Language Detection\n",
    "    \n",
    "    Features:\n",
    "    1. Physics-based acoustic analysis\n",
    "    2. Deep Learning deepfake detection\n",
    "    3. Language identification using Whisper (focus on Indian languages)\n",
    "    4. Auto-truncation to 30 seconds for faster processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        deepfake_model_path=\"garystafford/wav2vec2-deepfake-voice-detector\",\n",
    "        whisper_model_path=\"openai/whisper-base\",\n",
    "        physics_weight=0.4,\n",
    "        dl_weight=0.6,\n",
    "        use_local_deepfake_model=False,\n",
    "        use_local_whisper_model=False,\n",
    "        max_audio_duration=30  # seconds\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid detector\n",
    "        \n",
    "        Args:\n",
    "            deepfake_model_path: Path to deepfake detection model\n",
    "            whisper_model_path: Path to Whisper model for language detection\n",
    "            physics_weight: Weight for physics score (0-1)\n",
    "            dl_weight: Weight for DL score (0-1)\n",
    "            use_local_deepfake_model: Whether to load deepfake model from local path\n",
    "            use_local_whisper_model: Whether to load Whisper from local path\n",
    "            max_audio_duration: Maximum audio duration to process (seconds)\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_duration = max_audio_duration\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total_weight\n",
    "        self.dl_weight = dl_weight / total_weight\n",
    "        \n",
    "        print(f\"🔧 Initializing Hybrid Detector with Language Detection\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Physics Weight: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL Weight: {self.dl_weight*100:.0f}%\")\n",
    "        print(f\"   Max Audio Duration: {self.max_duration}s\")\n",
    "        \n",
    "        # --- LOAD DEEPFAKE DETECTION MODEL ---\n",
    "        try:\n",
    "            print(f\"📥 Loading deepfake detection model from '{deepfake_model_path}'...\")\n",
    "            \n",
    "            if use_local_deepfake_model:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(\n",
    "                    deepfake_model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "                    deepfake_model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "            else:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(deepfake_model_path)\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(deepfake_model_path)\n",
    "            \n",
    "            self.dl_model.to(self.device)\n",
    "            self.dl_model.eval()\n",
    "            self.dl_ready = True\n",
    "            print(\"✅ Deepfake Detection Model Loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  DL Model Load Failed: {e}\")\n",
    "            print(\"   Running in Physics-Only mode\")\n",
    "            self.dl_ready = False\n",
    "            self.dl_weight = 0\n",
    "            self.physics_weight = 1.0\n",
    "\n",
    "        # --- LOAD WHISPER FOR LANGUAGE DETECTION ---\n",
    "        try:\n",
    "            print(f\"📥 Loading Whisper model for language detection from '{whisper_model_path}'...\")\n",
    "            \n",
    "            if use_local_whisper_model:\n",
    "                self.whisper_processor = WhisperProcessor.from_pretrained(\n",
    "                    whisper_model_path,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "                    whisper_model_path,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            else:\n",
    "                self.whisper_processor = WhisperProcessor.from_pretrained(whisper_model_path)\n",
    "                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_path)\n",
    "            \n",
    "            self.whisper_model.to(self.device)\n",
    "            self.whisper_model.eval()\n",
    "            self.lang_ready = True\n",
    "            print(\"✅ Whisper Language Detection Model Loaded\")\n",
    "            \n",
    "            # Language code mapping for Indian languages and common languages\n",
    "            self.language_map = {\n",
    "                'hi': 'Hindi',\n",
    "                'bn': 'Bengali', \n",
    "                'te': 'Telugu',\n",
    "                'mr': 'Marathi',\n",
    "                'ta': 'Tamil',\n",
    "                'gu': 'Gujarati',\n",
    "                'kn': 'Kannada',\n",
    "                'ml': 'Malayalam',\n",
    "                'or': 'Odia',\n",
    "                'pa': 'Punjabi',\n",
    "                'as': 'Assamese',\n",
    "                'ur': 'Urdu',\n",
    "                'en': 'English',\n",
    "                'ne': 'Nepali',\n",
    "                'si': 'Sinhala',\n",
    "                'sa': 'Sanskrit',\n",
    "                'sd': 'Sindhi',\n",
    "                'ks': 'Kashmiri'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Whisper Model Load Failed: {e}\")\n",
    "            print(\"   Running without language detection\")\n",
    "            self.lang_ready = False\n",
    "\n",
    "        # --- PHYSICS ENGINE PARAMETERS ---\n",
    "        self.CV_AI_THRESHOLD = 0.20\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32\n",
    "        self.INTENSITY_MIN_STD = 0.05\n",
    "        self.INTENSITY_MAX_STD = 0.15\n",
    "        \n",
    "        print(\"✅ Hybrid Detector Ready\\n\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # HELPER: Audio Preprocessing\n",
    "    # ==========================================================\n",
    "    def preprocess_audio(self, audio_path, target_sr=16000):\n",
    "        \"\"\"\n",
    "        Load and preprocess audio:\n",
    "        1. Load audio\n",
    "        2. Convert to mono\n",
    "        3. Truncate to max_duration if needed\n",
    "        4. Resample to target_sr\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            target_sr: Target sample rate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (waveform_array, sample_rate, duration, was_truncated)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            \n",
    "            # Calculate duration\n",
    "            duration = len(y) / sr\n",
    "            was_truncated = False\n",
    "            \n",
    "            # Truncate if longer than max_duration\n",
    "            if duration > self.max_duration:\n",
    "                print(f\"   ⚠️  Audio is {duration:.1f}s, truncating to {self.max_duration}s\")\n",
    "                max_samples = int(self.max_duration * sr)\n",
    "                y = y[:max_samples]\n",
    "                duration = self.max_duration\n",
    "                was_truncated = True\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != target_sr:\n",
    "                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "                sr = target_sr\n",
    "            \n",
    "            return y, sr, duration, was_truncated\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to preprocess audio: {str(e)}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # HELPER: Base64 Decoding\n",
    "    # ==========================================================\n",
    "    def decode_base64_audio(self, base64_string):\n",
    "        \"\"\"\n",
    "        Decode base64 audio and save to temporary file\n",
    "        \n",
    "        Args:\n",
    "            base64_string: Base64 encoded audio data\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to temporary audio file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode base64\n",
    "            audio_data = base64.b64decode(base64_string)\n",
    "            \n",
    "            # Create temporary file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
    "            temp_file.write(audio_data)\n",
    "            temp_file.close()\n",
    "            \n",
    "            return temp_file.name\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to decode base64 audio: {str(e)}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # LANGUAGE DETECTION (FIXED)\n",
    "    # ==========================================================\n",
    "    def detect_language(self, audio_path):\n",
    "        \"\"\"\n",
    "        Detect language using Whisper model - FIXED VERSION\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            \n",
    "        Returns:\n",
    "            str: Detected language name\n",
    "        \"\"\"\n",
    "        if not self.lang_ready:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess audio for Whisper (uses 16kHz)\n",
    "            # Use first 30 seconds for language detection\n",
    "            audio, sr = librosa.load(audio_path, sr=16000, mono=True, duration=30)\n",
    "            \n",
    "            # Process audio with Whisper processor\n",
    "            input_features = self.whisper_processor(\n",
    "                audio,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            \n",
    "            input_features = input_features.to(self.device)\n",
    "            \n",
    "            # Whisper language detection using forced_decoder_ids\n",
    "            with torch.no_grad():\n",
    "                # Generate with language detection enabled\n",
    "                # Set task to \"transcribe\" and let Whisper detect language\n",
    "                generated_ids = self.whisper_model.generate(\n",
    "                    input_features,\n",
    "                    task=\"transcribe\",\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "                \n",
    "                # Decode the output\n",
    "                transcription = self.whisper_processor.batch_decode(\n",
    "                    generated_ids.sequences,\n",
    "                    skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                # Extract language from generated tokens\n",
    "                # Whisper embeds language token at the beginning\n",
    "                full_output = self.whisper_processor.batch_decode(\n",
    "                    generated_ids.sequences,\n",
    "                    skip_special_tokens=False\n",
    "                )[0]\n",
    "                \n",
    "                # Parse language from special tokens\n",
    "                # Format: <|startoftranscript|><|en|><|transcribe|>...\n",
    "                detected_lang = None\n",
    "                \n",
    "                # Look for language tokens in the format <|xx|>\n",
    "                import re\n",
    "                lang_pattern = r'<\\|([a-z]{2})\\|>'\n",
    "                matches = re.findall(lang_pattern, full_output)\n",
    "                \n",
    "                if matches:\n",
    "                    # First match after startoftranscript is usually the language\n",
    "                    for match in matches:\n",
    "                        if match in self.language_map:\n",
    "                            detected_lang = match\n",
    "                            break\n",
    "                \n",
    "                if detected_lang:\n",
    "                    lang_name = self.language_map.get(detected_lang, detected_lang.upper())\n",
    "                    print(f\"   🌐 Detected Language: {lang_name} ({detected_lang})\")\n",
    "                    return lang_name\n",
    "                else:\n",
    "                    # Fallback: Try alternate method using model's internal language detection\n",
    "                    # This method uses the log probabilities\n",
    "                    print(f\"   🌐 Language: Analyzing transcription...\")\n",
    "                    \n",
    "                    # Simple heuristic: if transcription has content, likely English or detected language\n",
    "                    if len(transcription.strip()) > 0:\n",
    "                        # Default to English if we can transcribe but can't detect language\n",
    "                        print(f\"   🌐 Detected Language: English (default)\")\n",
    "                        return \"English\"\n",
    "                    else:\n",
    "                        return \"Unknown\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Language detection error: {str(e)}\")\n",
    "            # Fallback: Try simple language detection based on transcription\n",
    "            try:\n",
    "                # Simplified approach\n",
    "                audio, sr = librosa.load(audio_path, sr=16000, mono=True, duration=30)\n",
    "                input_features = self.whisper_processor(\n",
    "                    audio,\n",
    "                    sampling_rate=16000,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_features.to(self.device)\n",
    "                \n",
    "                # Just generate transcription\n",
    "                with torch.no_grad():\n",
    "                    predicted_ids = self.whisper_model.generate(input_features)\n",
    "                    transcription = self.whisper_processor.batch_decode(\n",
    "                        predicted_ids, \n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                \n",
    "                # If we got transcription, assume English\n",
    "                if len(transcription.strip()) > 0:\n",
    "                    print(f\"   🌐 Detected Language: English (from transcription)\")\n",
    "                    return \"English\"\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return \"Unknown\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: PHYSICS ENGINE\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\"Linear interpolation for scoring\"\"\"\n",
    "        if val <= min_val:\n",
    "            return 1.0\n",
    "        if val >= max_val:\n",
    "            return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using physics-based acoustic features\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, method, features_dict)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and preprocess audio\n",
    "            y, sr, duration, was_truncated = self.preprocess_audio(audio_path, target_sr=None)\n",
    "            \n",
    "            # Robust pitch tracking using PYIN\n",
    "            f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "                y, \n",
    "                fmin=50, \n",
    "                fmax=400, \n",
    "                sr=sr\n",
    "            )\n",
    "            valid_f0 = f0[~np.isnan(f0)]\n",
    "            \n",
    "            if len(valid_f0) == 0:\n",
    "                return 0.0, \"No voice detected\", {'duration': duration, 'was_truncated': was_truncated}\n",
    "\n",
    "            # Extract acoustic features\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            # Calculate feature metrics\n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch if mean_pitch > 0 else 0,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid),\n",
    "                'mean_pitch': mean_pitch,\n",
    "                'std_pitch': std_pitch,\n",
    "                'duration': duration,\n",
    "                'was_truncated': was_truncated\n",
    "            }\n",
    "\n",
    "            # Individual feature scores (higher = more AI-like)\n",
    "            intensity_score = self.get_linear_score(\n",
    "                feats['intensity_std'], \n",
    "                self.INTENSITY_MIN_STD, \n",
    "                self.INTENSITY_MAX_STD\n",
    "            )\n",
    "            \n",
    "            pitch_score = self.get_linear_score(\n",
    "                feats['pitch_cv'], \n",
    "                self.CV_AI_THRESHOLD, \n",
    "                self.CV_HUMAN_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            skew_score = self.get_linear_score(\n",
    "                abs(feats['freq_skew']), \n",
    "                0.1, \n",
    "                1.0\n",
    "            )\n",
    "\n",
    "            # Weighted combination\n",
    "            W_INTENSITY = 0.40\n",
    "            W_PITCH = 0.40\n",
    "            W_SKEW = 0.20\n",
    "            \n",
    "            base_score = (\n",
    "                intensity_score * W_INTENSITY + \n",
    "                pitch_score * W_PITCH + \n",
    "                skew_score * W_SKEW\n",
    "            )\n",
    "\n",
    "            # Synergy bonus: if both intensity and pitch are suspicious\n",
    "            if intensity_score > 0.4 and pitch_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {'duration': 0, 'was_truncated': False}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: DEEP LEARNING ENGINE\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using deep learning model\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, label)\n",
    "        \"\"\"\n",
    "        if not self.dl_ready:\n",
    "            return 0.0, \"Model not loaded\"\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess audio\n",
    "            waveform_np, sr, duration, was_truncated = self.preprocess_audio(audio_path, target_sr=16000)\n",
    "\n",
    "            # Process with feature extractor\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform_np,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.dl_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            # Get predictions\n",
    "            # Class 0: Real, Class 1: Fake\n",
    "            prob_real = probs[0][0].item()\n",
    "            prob_fake = probs[0][1].item()\n",
    "            \n",
    "            # AI score is the fake probability\n",
    "            ai_score = prob_fake\n",
    "            \n",
    "            label = \"Fake/Deepfake\" if prob_fake > 0.5 else \"Real/Human\"\n",
    "\n",
    "            return round(ai_score, 3), label\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: EXPLANATION GENERATOR\n",
    "    # ==========================================================\n",
    "    def generate_explanation(self, final_score, phys_score, dl_score, dl_label, phys_feats):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the classification\n",
    "        \n",
    "        Returns:\n",
    "            str: Explanation text\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        if final_score > 0.55:\n",
    "            # AI GENERATED\n",
    "            \n",
    "            # Deep Learning contributions\n",
    "            if dl_score > 0.55 and self.dl_ready:\n",
    "                if \"Fake\" in dl_label or \"Deepfake\" in dl_label:\n",
    "                    explanations.append(\n",
    "                        f\"Deep learning model detected synthetic voice patterns \"\n",
    "                        f\"(confidence: {dl_score*100:.1f}%)\"\n",
    "                    )\n",
    "            \n",
    "            # Physics contributions\n",
    "            if phys_score > 0.55:\n",
    "                p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                i_std = phys_feats.get('intensity_std', 0)\n",
    "                \n",
    "                if i_std < 0.06:\n",
    "                    explanations.append(\n",
    "                        f\"Unnaturally consistent energy levels detected \"\n",
    "                        f\"(std: {i_std:.3f}, expected: >0.06)\"\n",
    "                    )\n",
    "                \n",
    "                if p_cv < 0.22:\n",
    "                    explanations.append(\n",
    "                        f\"Robotic pitch modulation patterns \"\n",
    "                        f\"(CV: {p_cv:.2f}, expected: >0.22)\"\n",
    "                    )\n",
    "                \n",
    "                if not explanations or (i_std >= 0.06 and p_cv >= 0.22):\n",
    "                    explanations.append(\n",
    "                        \"Acoustic parameters lack natural human variability\"\n",
    "                    )\n",
    "            \n",
    "            if not explanations:\n",
    "                explanations.append(\n",
    "                    \"Voice exhibits characteristics consistent with AI generation\"\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            # HUMAN\n",
    "            explanations.append(\n",
    "                \"Voice exhibits natural acoustic variability and human speech characteristics\"\n",
    "            )\n",
    "        \n",
    "        return \"; \".join(explanations)\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART D: MAIN ANALYSIS FUNCTION\n",
    "    # ==========================================================\n",
    "    def analyze(self, audio_input, input_type=\"file\"):\n",
    "        \"\"\"\n",
    "        Main analysis function with configurable input types\n",
    "        \n",
    "        Args:\n",
    "            audio_input: Either file path or base64 string\n",
    "            input_type: \"file\" or \"base64\"\n",
    "            \n",
    "        Returns:\n",
    "            dict: Analysis results following API response format\n",
    "        \"\"\"\n",
    "        temp_file = None\n",
    "        \n",
    "        try:\n",
    "            # Handle input type\n",
    "            if input_type == \"base64\":\n",
    "                temp_file = self.decode_base64_audio(audio_input)\n",
    "                audio_path = temp_file\n",
    "            elif input_type == \"file\":\n",
    "                audio_path = audio_input\n",
    "                if not os.path.exists(audio_path):\n",
    "                    return {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": f\"Audio file not found: {audio_path}\"\n",
    "                    }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"Invalid input_type: {input_type}. Use 'file' or 'base64'\"\n",
    "                }\n",
    "\n",
    "            print(f\"🎵 Analyzing: {os.path.basename(audio_path)}\")\n",
    "\n",
    "            # 1. Detect Language\n",
    "            detected_language = self.detect_language(audio_path)\n",
    "\n",
    "            # 2. Run Physics Analysis\n",
    "            phys_score, phys_method, phys_feats = self.get_physics_score(audio_path)\n",
    "            \n",
    "            # 3. Run Deep Learning Analysis\n",
    "            dl_score, dl_label = self.get_dl_score(audio_path)\n",
    "\n",
    "            # 4. Calculate weighted ensemble score\n",
    "            final_score = (\n",
    "                self.physics_weight * phys_score + \n",
    "                self.dl_weight * dl_score\n",
    "            )\n",
    "            \n",
    "            # Round to 2 decimal places\n",
    "            final_score = round(final_score, 2)\n",
    "            \n",
    "            # 5. Determine classification\n",
    "            classification = \"AI_GENERATED\" if final_score > 0.55 else \"HUMAN\"\n",
    "            \n",
    "            # 6. Generate explanation\n",
    "            explanation = self.generate_explanation(\n",
    "                final_score, \n",
    "                phys_score, \n",
    "                dl_score, \n",
    "                dl_label, \n",
    "                phys_feats\n",
    "            )\n",
    "\n",
    "            # 7. Return API-compliant response\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"language\": detected_language,\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": final_score,\n",
    "                \"explanation\": explanation,\n",
    "                \"debug\": {\n",
    "                    \"physics_score\": phys_score,\n",
    "                    \"dl_score\": dl_score,\n",
    "                    \"dl_label\": dl_label,\n",
    "                    \"physics_weight\": f\"{self.physics_weight*100:.0f}%\",\n",
    "                    \"dl_weight\": f\"{self.dl_weight*100:.0f}%\",\n",
    "                    \"audio_duration\": phys_feats.get('duration', 0),\n",
    "                    \"was_truncated\": phys_feats.get('was_truncated', False),\n",
    "                    \"physics_features\": {k: v for k, v in phys_feats.items() if k not in ['duration', 'was_truncated']}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            if temp_file and os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.unlink(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # ==========================================================\n",
    "    # UTILITY: Update Weights\n",
    "    # ==========================================================\n",
    "    def update_weights(self, physics_weight, dl_weight):\n",
    "        \"\"\"\n",
    "        Update ensemble weights dynamically\n",
    "        \n",
    "        Args:\n",
    "            physics_weight: New physics weight (0-1)\n",
    "            dl_weight: New DL weight (0-1)\n",
    "        \"\"\"\n",
    "        total = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total\n",
    "        self.dl_weight = dl_weight / total\n",
    "        \n",
    "        print(f\"⚙️  Weights updated:\")\n",
    "        print(f\"   Physics: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL: {self.dl_weight*100:.0f}%\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# USAGE EXAMPLES\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Initialize with 40-60 split (Physics-DL)\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXAMPLE 1: Initialize Hybrid Detector with Language Detection\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    detector = HybridEnsembleDetector(\n",
    "        deepfake_model_path=\"wav2vec2-deepfake-voice-detector\",\n",
    "        whisper_model_path=\"openai/whisper-base\",\n",
    "        physics_weight=0.4,\n",
    "        dl_weight=0.6,\n",
    "        use_local_deepfake_model=True,\n",
    "        use_local_whisper_model=False,\n",
    "        max_audio_duration=30  # Truncate to 30 seconds\n",
    "    )\n",
    "    \n",
    "    # Test with file path\n",
    "    audio_file = r\"sample voice 1.mp3\"\n",
    "    result = detector.analyze(audio_file, input_type=\"file\")\n",
    "    \n",
    "    print(f\"\\n📊 Result:\")\n",
    "    print(f\"   Status: {result['status']}\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"   Language: {result['language']}\")\n",
    "        print(f\"   Classification: {result['classification']}\")\n",
    "        print(f\"   Confidence: {result['confidenceScore']}\")\n",
    "        print(f\"   Explanation: {result['explanation']}\")\n",
    "        print(f\"\\n🔍 Debug Info:\")\n",
    "        for key, val in result.get('debug', {}).items():\n",
    "            if key != 'physics_features':\n",
    "                print(f\"   {key}: {val}\")\n",
    "    else:\n",
    "        print(f\"   Error: {result.get('error')}\")\n",
    "    \n",
    "    \n",
    "    # Example 2: Batch processing\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Batch Processing with Language Detection\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_files = [\n",
    "        r\"voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "        r\"clova.mp3\",\n",
    "        r\"sample voice 1.mp3\",\n",
    "        r\"voice_preview_mukundan - formal and clear.mp3\",\n",
    "        r\"voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "        r\"medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "        r\"voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Filename':<40} {'Language':<15} {'Classification':<15} {'Confidence'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            result = detector.analyze(file_path, input_type=\"file\")\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                filename = os.path.basename(file_path)[:37] + \"...\"\n",
    "                print(f\"{filename:<40} {result['language']:<15} {result['classification']:<15} {result['confidenceScore']:.2f}\")\n",
    "                print(f\"\\n🔍 Debug Info:\")\n",
    "                for key, val in result.get('debug', {}).items():\n",
    "                    print(f\"   {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Initialize Hybrid Detector with Language Detection\n",
      "======================================================================\n",
      "🔧 Initializing Hybrid Detector with Language Detection\n",
      "   Device: cpu\n",
      "   Physics Weight: 40%\n",
      "   DL Weight: 60%\n",
      "   Max Audio Duration: 30s\n",
      "📥 Loading deepfake detection model from 'wav2vec2-deepfake-voice-detector'...\n",
      "✅ Deepfake Detection Model Loaded\n",
      "📥 Loading Whisper model for language detection from 'openai/whisper-base'...\n",
      "✅ Whisper Language Detection Model Loaded\n",
      "✅ Hybrid Detector Ready\n",
      "\n",
      "🎵 Analyzing: sample voice 1.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "   🔬 Running physics analysis on 23.6s audio at 44100Hz\n",
      "   🔬 Physics score: 0.861 (intensity:0.78, pitch:1.00)\n",
      "\n",
      "📊 Result:\n",
      "   Status: success\n",
      "   Language: English\n",
      "   Classification: AI_GENERATED\n",
      "   Confidence: 0.9399999976158142\n",
      "   Explanation: Deep learning model detected synthetic voice patterns (confidence: 98.5%); Robotic pitch modulation patterns (CV: 0.20, expected: >0.22)\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.8610000014305115\n",
      "   dl_score: 0.985\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 23.588571428571427\n",
      "   was_truncated: False\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Batch Processing with Language Detection\n",
      "======================================================================\n",
      "\n",
      "Filename                                 Language        Classification  Confidence\n",
      "--------------------------------------------------------------------------------\n",
      "🎵 Analyzing: voice_preview_faiq - standard, clear and neutral.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "   🔬 Running physics analysis on 8.7s audio at 44100Hz\n",
      "   🔬 Physics score: 0.935 (intensity:0.98, pitch:0.98)\n",
      "voice_preview_faiq - standard, clear ... Hindi           AI_GENERATED    0.97\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.935\n",
      "   dl_score: 0.996\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 8.672653061224489\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.2018661247976809), 'intensity_std': np.float32(0.052201804), 'freq_skew': np.float64(1.9400670286624346), 'mean_pitch': np.float64(102.85900030456028), 'std_pitch': np.float64(20.763747792045063)}\n",
      "🎵 Analyzing: clova.mp3\n",
      "   🌐 Detected Language: English (default)\n",
      "   🔬 Running physics analysis on 14.1s audio at 24000Hz\n",
      "   🔬 Physics score: 0.874 (intensity:0.81, pitch:1.00)\n",
      "clova.mp3...                             English         AI_GENERATED    0.84\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.8740000128746033\n",
      "   dl_score: 0.822\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 14.078166666666666\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.1750595894476743), 'intensity_std': np.float32(0.069120765), 'freq_skew': np.float64(1.2180965383818596), 'mean_pitch': np.float64(244.9748965075902), 'std_pitch': np.float64(42.88520480760524)}\n",
      "🎵 Analyzing: sample voice 1.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "   🔬 Running physics analysis on 23.6s audio at 44100Hz\n",
      "   🔬 Physics score: 0.861 (intensity:0.78, pitch:1.00)\n",
      "sample voice 1.mp3...                    English         AI_GENERATED    0.94\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.8610000014305115\n",
      "   dl_score: 0.985\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 23.588571428571427\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.1964107013866248), 'intensity_std': np.float32(0.07224538), 'freq_skew': np.float64(1.2587673386321117), 'mean_pitch': np.float64(121.2684505871028), 'std_pitch': np.float64(23.818421435882115)}\n",
      "🎵 Analyzing: voice_preview_mukundan - formal and clear.mp3\n",
      "   🌐 Detected Language: Tamil (ta)\n",
      "   🔬 Running physics analysis on 6.1s audio at 44100Hz\n",
      "   🔬 Physics score: 0.836 (intensity:1.00, pitch:0.72)\n",
      "voice_preview_mukundan - formal and c... Tamil           AI_GENERATED    0.93\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.836\n",
      "   dl_score: 0.994\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 6.086530612244898\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.23417541464654282), 'intensity_std': np.float32(0.0488326), 'freq_skew': np.float64(1.5337868334334925), 'mean_pitch': np.float64(125.71629334603644), 'std_pitch': np.float64(29.439665122134496)}\n",
      "🎵 Analyzing: voice_preview_kanika - soft, smooth and muffled.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "   🔬 Running physics analysis on 8.9s audio at 44100Hz\n",
      "   🔬 Physics score: 0.476 (intensity:0.98, pitch:0.21)\n",
      "voice_preview_kanika - soft, smooth a... Hindi           HUMAN           0.22\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.476\n",
      "   dl_score: 0.056\n",
      "   dl_label: Real/Human\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 8.855510204081632\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.2950107965050219), 'intensity_std': np.float32(0.05191575), 'freq_skew': np.float64(2.5388488531782025), 'mean_pitch': np.float64(237.2917362550618), 'std_pitch': np.float64(70.00362411666536)}\n",
      "🎵 Analyzing: medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\n",
      "   🌐 Detected Language: English (en)\n",
      "   🔬 Running physics analysis on 5.8s audio at 48000Hz\n",
      "   🔬 Physics score: 0.863 (intensity:0.83, pitch:0.76)\n",
      "medieval-gamer-voice-darkness-hunts-u... English         HUMAN           0.43\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.863\n",
      "   dl_score: 0.139\n",
      "   dl_label: Real/Human\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 5.76\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.22895445135268358), 'intensity_std': np.float32(0.06666476), 'freq_skew': np.float64(0.6549836083283521), 'mean_pitch': np.float64(94.7100781081681), 'std_pitch': np.float64(21.684293970825436)}\n",
      "🎵 Analyzing: voice_preview_tarini - soft, cheerful and expressive.mp3\n",
      "   🌐 Detected Language: Hindi (hi)\n",
      "   🔬 Running physics analysis on 7.6s audio at 44100Hz\n",
      "   🔬 Physics score: 0.731 (intensity:1.00, pitch:0.45)\n",
      "voice_preview_tarini - soft, cheerful... Hindi           AI_GENERATED    0.87\n",
      "\n",
      "🔍 Debug Info:\n",
      "   physics_score: 0.731\n",
      "   dl_score: 0.967\n",
      "   dl_label: Fake/Deepfake\n",
      "   physics_weight: 40%\n",
      "   dl_weight: 60%\n",
      "   audio_duration: 7.601632653061224\n",
      "   was_truncated: False\n",
      "   physics_features: {'pitch_cv': np.float64(0.26555274920036603), 'intensity_std': np.float32(0.043198153), 'freq_skew': np.float64(1.359989262538064), 'mean_pitch': np.float64(226.78289778423144), 'std_pitch': np.float64(60.222821978228254)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "import warnings\n",
    "\n",
    "# Suppress librosa warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HybridEnsembleDetector:\n",
    "    \"\"\"\n",
    "    Hybrid AI Voice Detection System with Language Detection\n",
    "    \n",
    "    Features:\n",
    "    1. Physics-based acoustic analysis\n",
    "    2. Deep Learning deepfake detection\n",
    "    3. Language identification using Whisper (focus on Indian languages)\n",
    "    4. Auto-truncation to 30 seconds for faster processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        deepfake_model_path=\"garystafford/wav2vec2-deepfake-voice-detector\",\n",
    "        whisper_model_path=\"openai/whisper-base\",\n",
    "        physics_weight=0.4,\n",
    "        dl_weight=0.6,\n",
    "        use_local_deepfake_model=False,\n",
    "        use_local_whisper_model=False,\n",
    "        max_audio_duration=30  # seconds\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid detector\n",
    "        \n",
    "        Args:\n",
    "            deepfake_model_path: Path to deepfake detection model\n",
    "            whisper_model_path: Path to Whisper model for language detection\n",
    "            physics_weight: Weight for physics score (0-1)\n",
    "            dl_weight: Weight for DL score (0-1)\n",
    "            use_local_deepfake_model: Whether to load deepfake model from local path\n",
    "            use_local_whisper_model: Whether to load Whisper from local path\n",
    "            max_audio_duration: Maximum audio duration to process (seconds)\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_duration = max_audio_duration\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total_weight\n",
    "        self.dl_weight = dl_weight / total_weight\n",
    "        \n",
    "        print(f\"🔧 Initializing Hybrid Detector with Language Detection\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Physics Weight: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL Weight: {self.dl_weight*100:.0f}%\")\n",
    "        print(f\"   Max Audio Duration: {self.max_duration}s\")\n",
    "        \n",
    "        # --- LOAD DEEPFAKE DETECTION MODEL ---\n",
    "        try:\n",
    "            print(f\"📥 Loading deepfake detection model from '{deepfake_model_path}'...\")\n",
    "            \n",
    "            if use_local_deepfake_model:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(\n",
    "                    deepfake_model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "                    deepfake_model_path, \n",
    "                    local_files_only=True\n",
    "                )\n",
    "            else:\n",
    "                self.dl_model = AutoModelForAudioClassification.from_pretrained(deepfake_model_path)\n",
    "                self.feature_extractor = AutoFeatureExtractor.from_pretrained(deepfake_model_path)\n",
    "            \n",
    "            self.dl_model.to(self.device)\n",
    "            self.dl_model.eval()\n",
    "            self.dl_ready = True\n",
    "            print(\"✅ Deepfake Detection Model Loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  DL Model Load Failed: {e}\")\n",
    "            print(\"   Running in Physics-Only mode\")\n",
    "            self.dl_ready = False\n",
    "            self.dl_weight = 0\n",
    "            self.physics_weight = 1.0\n",
    "\n",
    "        # --- LOAD WHISPER FOR LANGUAGE DETECTION ---\n",
    "        try:\n",
    "            print(f\"📥 Loading Whisper model for language detection from '{whisper_model_path}'...\")\n",
    "            \n",
    "            if use_local_whisper_model:\n",
    "                self.whisper_processor = WhisperProcessor.from_pretrained(\n",
    "                    whisper_model_path,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "                    whisper_model_path,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            else:\n",
    "                self.whisper_processor = WhisperProcessor.from_pretrained(whisper_model_path)\n",
    "                self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_path)\n",
    "            \n",
    "            self.whisper_model.to(self.device)\n",
    "            self.whisper_model.eval()\n",
    "            self.lang_ready = True\n",
    "            print(\"✅ Whisper Language Detection Model Loaded\")\n",
    "            \n",
    "            # Language code mapping for Indian languages and common languages\n",
    "            self.language_map = {\n",
    "                'hi': 'Hindi',\n",
    "                'bn': 'Bengali', \n",
    "                'te': 'Telugu',\n",
    "                'mr': 'Marathi',\n",
    "                'ta': 'Tamil',\n",
    "                'gu': 'Gujarati',\n",
    "                'kn': 'Kannada',\n",
    "                'ml': 'Malayalam',\n",
    "                'or': 'Odia',\n",
    "                'pa': 'Punjabi',\n",
    "                'as': 'Assamese',\n",
    "                'ur': 'Urdu',\n",
    "                'en': 'English',\n",
    "                'ne': 'Nepali',\n",
    "                'si': 'Sinhala',\n",
    "                'sa': 'Sanskrit',\n",
    "                'sd': 'Sindhi',\n",
    "                'ks': 'Kashmiri'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Whisper Model Load Failed: {e}\")\n",
    "            print(\"   Running without language detection\")\n",
    "            self.lang_ready = False\n",
    "\n",
    "        # --- PHYSICS ENGINE PARAMETERS ---\n",
    "        self.CV_AI_THRESHOLD = 0.20\n",
    "        self.CV_HUMAN_THRESHOLD = 0.32\n",
    "        self.INTENSITY_MIN_STD = 0.05\n",
    "        self.INTENSITY_MAX_STD = 0.15\n",
    "        \n",
    "        print(\"✅ Hybrid Detector Ready\\n\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # HELPER: Audio Preprocessing\n",
    "    # ==========================================================\n",
    "    def preprocess_audio(self, audio_path, target_sr=16000):\n",
    "        \"\"\"\n",
    "        Load and preprocess audio:\n",
    "        1. Load audio\n",
    "        2. Convert to mono\n",
    "        3. Truncate to max_duration if needed\n",
    "        4. Resample to target_sr\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            target_sr: Target sample rate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (waveform_array, sample_rate, duration, was_truncated)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            \n",
    "            # Calculate duration\n",
    "            duration = len(y) / sr\n",
    "            was_truncated = False\n",
    "            \n",
    "            # Truncate if longer than max_duration\n",
    "            if duration > self.max_duration:\n",
    "                print(f\"   ⚠️  Audio is {duration:.1f}s, truncating to {self.max_duration}s\")\n",
    "                max_samples = int(self.max_duration * sr)\n",
    "                y = y[:max_samples]\n",
    "                duration = self.max_duration\n",
    "                was_truncated = True\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != target_sr:\n",
    "                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "                sr = target_sr\n",
    "            \n",
    "            return y, sr, duration, was_truncated\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to preprocess audio: {str(e)}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # HELPER: Base64 Decoding\n",
    "    # ==========================================================\n",
    "    def decode_base64_audio(self, base64_string):\n",
    "        \"\"\"\n",
    "        Decode base64 audio and save to temporary file\n",
    "        \n",
    "        Args:\n",
    "            base64_string: Base64 encoded audio data\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to temporary audio file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode base64\n",
    "            audio_data = base64.b64decode(base64_string)\n",
    "            \n",
    "            # Create temporary file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
    "            temp_file.write(audio_data)\n",
    "            temp_file.close()\n",
    "            \n",
    "            return temp_file.name\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to decode base64 audio: {str(e)}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # LANGUAGE DETECTION\n",
    "    # ==========================================================\n",
    "    def detect_language(self, audio_path):\n",
    "        \"\"\"\n",
    "        Detect language using Whisper model\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            \n",
    "        Returns:\n",
    "            str: Detected language name\n",
    "        \"\"\"\n",
    "        if not self.lang_ready:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess audio for Whisper (uses 16kHz)\n",
    "            # Use first 30 seconds for language detection\n",
    "            audio, sr = librosa.load(audio_path, sr=16000, mono=True, duration=30)\n",
    "            \n",
    "            # Process audio with Whisper processor\n",
    "            input_features = self.whisper_processor(\n",
    "                audio,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            \n",
    "            input_features = input_features.to(self.device)\n",
    "            \n",
    "            # Whisper language detection using forced_decoder_ids\n",
    "            with torch.no_grad():\n",
    "                # Generate with language detection enabled\n",
    "                generated_ids = self.whisper_model.generate(\n",
    "                    input_features,\n",
    "                    task=\"transcribe\",\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "                \n",
    "                # Decode the output\n",
    "                full_output = self.whisper_processor.batch_decode(\n",
    "                    generated_ids.sequences,\n",
    "                    skip_special_tokens=False\n",
    "                )[0]\n",
    "                \n",
    "                # Parse language from special tokens\n",
    "                # Format: <|startoftranscript|><|en|><|transcribe|>...\n",
    "                detected_lang = None\n",
    "                \n",
    "                # Look for language tokens in the format <|xx|>\n",
    "                import re\n",
    "                lang_pattern = r'<\\|([a-z]{2})\\|>'\n",
    "                matches = re.findall(lang_pattern, full_output)\n",
    "                \n",
    "                if matches:\n",
    "                    # First match after startoftranscript is usually the language\n",
    "                    for match in matches:\n",
    "                        if match in self.language_map:\n",
    "                            detected_lang = match\n",
    "                            break\n",
    "                \n",
    "                if detected_lang:\n",
    "                    lang_name = self.language_map.get(detected_lang, detected_lang.upper())\n",
    "                    print(f\"   🌐 Detected Language: {lang_name} ({detected_lang})\")\n",
    "                    return lang_name\n",
    "                else:\n",
    "                    # Fallback: if transcription successful, assume English\n",
    "                    transcription = self.whisper_processor.batch_decode(\n",
    "                        generated_ids.sequences,\n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                    \n",
    "                    if len(transcription.strip()) > 0:\n",
    "                        print(f\"   🌐 Detected Language: English (default)\")\n",
    "                        return \"English\"\n",
    "                    else:\n",
    "                        return \"Unknown\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Language detection error: {str(e)}\")\n",
    "            return \"Unknown\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART A: PHYSICS ENGINE (FIXED)\n",
    "    # ==========================================================\n",
    "    def get_linear_score(self, val, min_val, max_val):\n",
    "        \"\"\"Linear interpolation for scoring\"\"\"\n",
    "        if val <= min_val:\n",
    "            return 1.0\n",
    "        if val >= max_val:\n",
    "            return 0.0\n",
    "        return 1.0 - ((val - min_val) / (max_val - min_val))\n",
    "\n",
    "    def get_physics_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using physics-based acoustic features\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, method, features_dict)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio at NATIVE sample rate (don't resample for physics analysis)\n",
    "            y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            \n",
    "            # Calculate original duration\n",
    "            duration = len(y) / sr\n",
    "            was_truncated = False\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if duration > self.max_duration:\n",
    "                max_samples = int(self.max_duration * sr)\n",
    "                y = y[:max_samples]\n",
    "                duration = self.max_duration\n",
    "                was_truncated = True\n",
    "            \n",
    "            print(f\"   🔬 Running physics analysis on {duration:.1f}s audio at {sr}Hz\")\n",
    "            \n",
    "            # Robust pitch tracking using PYIN\n",
    "            try:\n",
    "                f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "                    y, \n",
    "                    fmin=librosa.note_to_hz('C2'),  # ~65 Hz\n",
    "                    fmax=librosa.note_to_hz('C7'),  # ~2093 Hz\n",
    "                    sr=sr,\n",
    "                    frame_length=2048\n",
    "                )\n",
    "                valid_f0 = f0[~np.isnan(f0)]\n",
    "            except Exception as pitch_error:\n",
    "                print(f\"   ⚠️  Pitch detection failed: {pitch_error}, using fallback method\")\n",
    "                # Fallback: use simpler pitch detection\n",
    "                valid_f0 = np.array([])\n",
    "            \n",
    "            if len(valid_f0) < 10:  # Need at least 10 valid pitch points\n",
    "                print(f\"   ⚠️  Insufficient pitch data ({len(valid_f0)} points), using alternative features\")\n",
    "                # Fall back to non-pitch features\n",
    "                rms = librosa.feature.rms(y=y)[0]\n",
    "                centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "                zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "                \n",
    "                feats = {\n",
    "                    'pitch_cv': 0.25,  # Neutral value\n",
    "                    'intensity_std': np.std(rms),\n",
    "                    'freq_skew': stats.skew(centroid),\n",
    "                    'zcr_std': np.std(zcr),\n",
    "                    'mean_pitch': 0,\n",
    "                    'std_pitch': 0,\n",
    "                    'duration': duration,\n",
    "                    'was_truncated': was_truncated\n",
    "                }\n",
    "                \n",
    "                # Score based on available features\n",
    "                intensity_score = self.get_linear_score(\n",
    "                    feats['intensity_std'], \n",
    "                    self.INTENSITY_MIN_STD, \n",
    "                    self.INTENSITY_MAX_STD\n",
    "                )\n",
    "                \n",
    "                zcr_score = self.get_linear_score(\n",
    "                    feats['zcr_std'],\n",
    "                    0.01,\n",
    "                    0.08\n",
    "                )\n",
    "                \n",
    "                skew_score = self.get_linear_score(\n",
    "                    abs(feats['freq_skew']), \n",
    "                    0.1, \n",
    "                    1.0\n",
    "                )\n",
    "                \n",
    "                # Weighted combination (no pitch)\n",
    "                final_score = (intensity_score * 0.5 + zcr_score * 0.2 + skew_score * 0.3)\n",
    "                \n",
    "                print(f\"   🔬 Physics score (no pitch): {final_score:.3f}\")\n",
    "                return round(final_score, 3), \"Physics Analysis (Limited)\", feats\n",
    "\n",
    "            # Full analysis with pitch\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "            \n",
    "            mean_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            \n",
    "            # Calculate feature metrics\n",
    "            feats = {\n",
    "                'pitch_cv': std_pitch / mean_pitch if mean_pitch > 0 else 0,\n",
    "                'intensity_std': np.std(rms),\n",
    "                'freq_skew': stats.skew(centroid),\n",
    "                'mean_pitch': mean_pitch,\n",
    "                'std_pitch': std_pitch,\n",
    "                'duration': duration,\n",
    "                'was_truncated': was_truncated\n",
    "            }\n",
    "\n",
    "            # Individual feature scores (higher = more AI-like)\n",
    "            intensity_score = self.get_linear_score(\n",
    "                feats['intensity_std'], \n",
    "                self.INTENSITY_MIN_STD, \n",
    "                self.INTENSITY_MAX_STD\n",
    "            )\n",
    "            \n",
    "            pitch_score = self.get_linear_score(\n",
    "                feats['pitch_cv'], \n",
    "                self.CV_AI_THRESHOLD, \n",
    "                self.CV_HUMAN_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            skew_score = self.get_linear_score(\n",
    "                abs(feats['freq_skew']), \n",
    "                0.1, \n",
    "                1.0\n",
    "            )\n",
    "\n",
    "            # Weighted combination\n",
    "            W_INTENSITY = 0.40\n",
    "            W_PITCH = 0.40\n",
    "            W_SKEW = 0.20\n",
    "            \n",
    "            base_score = (\n",
    "                intensity_score * W_INTENSITY + \n",
    "                pitch_score * W_PITCH + \n",
    "                skew_score * W_SKEW\n",
    "            )\n",
    "\n",
    "            # Synergy bonus: if both intensity and pitch are suspicious\n",
    "            if intensity_score > 0.4 and pitch_score > 0.4:\n",
    "                final_score = min(base_score + 0.15, 1.0)\n",
    "            else:\n",
    "                final_score = base_score\n",
    "\n",
    "            print(f\"   🔬 Physics score: {final_score:.3f} (intensity:{intensity_score:.2f}, pitch:{pitch_score:.2f})\")\n",
    "            return round(final_score, 3), \"Physics Analysis\", feats\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Physics analysis failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return 0.0, f\"Physics Error: {str(e)}\", {'duration': 0, 'was_truncated': False}\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART B: DEEP LEARNING ENGINE\n",
    "    # ==========================================================\n",
    "    def get_dl_score(self, audio_path):\n",
    "        \"\"\"\n",
    "        Analyze audio using deep learning model\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (ai_score, label)\n",
    "        \"\"\"\n",
    "        if not self.dl_ready:\n",
    "            return 0.0, \"Model not loaded\"\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess audio\n",
    "            waveform_np, sr, duration, was_truncated = self.preprocess_audio(audio_path, target_sr=16000)\n",
    "\n",
    "            # Process with feature extractor\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform_np,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.dl_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "            # Get predictions\n",
    "            # Class 0: Real, Class 1: Fake\n",
    "            prob_real = probs[0][0].item()\n",
    "            prob_fake = probs[0][1].item()\n",
    "            \n",
    "            # AI score is the fake probability\n",
    "            ai_score = prob_fake\n",
    "            \n",
    "            label = \"Fake/Deepfake\" if prob_fake > 0.5 else \"Real/Human\"\n",
    "\n",
    "            return round(ai_score, 3), label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ DL analysis failed: {str(e)}\")\n",
    "            return 0.0, f\"DL Error: {str(e)}\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART C: EXPLANATION GENERATOR\n",
    "    # ==========================================================\n",
    "    def generate_explanation(self, final_score, phys_score, dl_score, dl_label, phys_feats):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the classification\n",
    "        \n",
    "        Returns:\n",
    "            str: Explanation text\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        if final_score > 0.55:\n",
    "            # AI GENERATED\n",
    "            \n",
    "            # Deep Learning contributions\n",
    "            if dl_score > 0.55 and self.dl_ready:\n",
    "                if \"Fake\" in dl_label or \"Deepfake\" in dl_label:\n",
    "                    explanations.append(\n",
    "                        f\"Deep learning model detected synthetic voice patterns \"\n",
    "                        f\"(confidence: {dl_score*100:.1f}%)\"\n",
    "                    )\n",
    "            \n",
    "            # Physics contributions\n",
    "            if phys_score > 0.55:\n",
    "                p_cv = phys_feats.get('pitch_cv', 0)\n",
    "                i_std = phys_feats.get('intensity_std', 0)\n",
    "                \n",
    "                if i_std < 0.06:\n",
    "                    explanations.append(\n",
    "                        f\"Unnaturally consistent energy levels detected \"\n",
    "                        f\"(std: {i_std:.3f}, expected: >0.06)\"\n",
    "                    )\n",
    "                \n",
    "                if p_cv < 0.22 and p_cv > 0:\n",
    "                    explanations.append(\n",
    "                        f\"Robotic pitch modulation patterns \"\n",
    "                        f\"(CV: {p_cv:.2f}, expected: >0.22)\"\n",
    "                    )\n",
    "                \n",
    "                if not explanations or (i_std >= 0.06 and p_cv >= 0.22):\n",
    "                    explanations.append(\n",
    "                        \"Acoustic parameters lack natural human variability\"\n",
    "                    )\n",
    "            \n",
    "            if not explanations:\n",
    "                explanations.append(\n",
    "                    \"Voice exhibits characteristics consistent with AI generation\"\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            # HUMAN\n",
    "            explanations.append(\n",
    "                \"Voice exhibits natural acoustic variability and human speech characteristics\"\n",
    "            )\n",
    "        \n",
    "        return \"; \".join(explanations)\n",
    "\n",
    "    # ==========================================================\n",
    "    # PART D: MAIN ANALYSIS FUNCTION\n",
    "    # ==========================================================\n",
    "    def analyze(self, audio_input, input_type=\"file\"):\n",
    "        \"\"\"\n",
    "        Main analysis function with configurable input types\n",
    "        \n",
    "        Args:\n",
    "            audio_input: Either file path or base64 string\n",
    "            input_type: \"file\" or \"base64\"\n",
    "            \n",
    "        Returns:\n",
    "            dict: Analysis results following API response format\n",
    "        \"\"\"\n",
    "        temp_file = None\n",
    "        \n",
    "        try:\n",
    "            # Handle input type\n",
    "            if input_type == \"base64\":\n",
    "                temp_file = self.decode_base64_audio(audio_input)\n",
    "                audio_path = temp_file\n",
    "            elif input_type == \"file\":\n",
    "                audio_path = audio_input\n",
    "                if not os.path.exists(audio_path):\n",
    "                    return {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": f\"Audio file not found: {audio_path}\"\n",
    "                    }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"Invalid input_type: {input_type}. Use 'file' or 'base64'\"\n",
    "                }\n",
    "\n",
    "            print(f\"🎵 Analyzing: {os.path.basename(audio_path)}\")\n",
    "\n",
    "            # 1. Detect Language\n",
    "            detected_language = self.detect_language(audio_path)\n",
    "\n",
    "            # 2. Run Physics Analysis\n",
    "            phys_score, phys_method, phys_feats = self.get_physics_score(audio_path)\n",
    "            \n",
    "            # 3. Run Deep Learning Analysis\n",
    "            dl_score, dl_label = self.get_dl_score(audio_path)\n",
    "\n",
    "            # 4. Calculate weighted ensemble score\n",
    "            final_score = (\n",
    "                self.physics_weight * phys_score + \n",
    "                self.dl_weight * dl_score\n",
    "            )\n",
    "            \n",
    "            # Round to 2 decimal places\n",
    "            final_score = round(final_score, 2)\n",
    "            \n",
    "            # 5. Determine classification\n",
    "            classification = \"AI_GENERATED\" if final_score > 0.55 else \"HUMAN\"\n",
    "            \n",
    "            # 6. Generate explanation\n",
    "            explanation = self.generate_explanation(\n",
    "                final_score, \n",
    "                phys_score, \n",
    "                dl_score, \n",
    "                dl_label, \n",
    "                phys_feats\n",
    "            )\n",
    "\n",
    "            # 7. Return API-compliant response\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"language\": detected_language,\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": final_score,\n",
    "                \"explanation\": explanation,\n",
    "                \"debug\": {\n",
    "                    \"physics_score\": phys_score,\n",
    "                    \"dl_score\": dl_score,\n",
    "                    \"dl_label\": dl_label,\n",
    "                    \"physics_weight\": f\"{self.physics_weight*100:.0f}%\",\n",
    "                    \"dl_weight\": f\"{self.dl_weight*100:.0f}%\",\n",
    "                    \"audio_duration\": phys_feats.get('duration', 0),\n",
    "                    \"was_truncated\": phys_feats.get('was_truncated', False),\n",
    "                    \"physics_features\": {k: v for k, v in phys_feats.items() if k not in ['duration', 'was_truncated']}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            if temp_file and os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.unlink(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # ==========================================================\n",
    "    # UTILITY: Update Weights\n",
    "    # ==========================================================\n",
    "    def update_weights(self, physics_weight, dl_weight):\n",
    "        \"\"\"\n",
    "        Update ensemble weights dynamically\n",
    "        \n",
    "        Args:\n",
    "            physics_weight: New physics weight (0-1)\n",
    "            dl_weight: New DL weight (0-1)\n",
    "        \"\"\"\n",
    "        total = physics_weight + dl_weight\n",
    "        self.physics_weight = physics_weight / total\n",
    "        self.dl_weight = dl_weight / total\n",
    "        \n",
    "        print(f\"⚙️  Weights updated:\")\n",
    "        print(f\"   Physics: {self.physics_weight*100:.0f}%\")\n",
    "        print(f\"   DL: {self.dl_weight*100:.0f}%\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# USAGE EXAMPLES\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Initialize with 40-60 split (Physics-DL)\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXAMPLE 1: Initialize Hybrid Detector with Language Detection\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    detector = HybridEnsembleDetector(\n",
    "        deepfake_model_path=\"garystafford/wav2vec2-deepfake-voice-detector\",\n",
    "        whisper_model_path=\"openai/whisper-base\",\n",
    "        physics_weight=0.4,\n",
    "        dl_weight=0.6,\n",
    "        use_local_deepfake_model=False,\n",
    "        use_local_whisper_model=False,\n",
    "        max_audio_duration=30  # Truncate to 30 seconds\n",
    "    )\n",
    "    \n",
    "    # Test with file path\n",
    "    audio_file = r\"test_audio\\human\\medieval-gamer-voice-you-can-view-our-website-at-the-link-below-228410.mp3\"\n",
    "    result = detector.analyze(audio_file, input_type=\"file\")\n",
    "    \n",
    "    print(f\"\\n📊 Result:\")\n",
    "    print(f\"   Status: {result['status']}\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"   Language: {result['language']}\")\n",
    "        print(f\"   Classification: {result['classification']}\")\n",
    "        print(f\"   Confidence: {result['confidenceScore']}\")\n",
    "        print(f\"   Explanation: {result['explanation']}\")\n",
    "        print(f\"\\n🔍 Debug Info:\")\n",
    "        for key, val in result.get('debug', {}).items():\n",
    "            if key != 'physics_features':\n",
    "                print(f\"   {key}: {val}\")\n",
    "    else:\n",
    "        print(f\"   Error: {result.get('error')}\")\n",
    "    \n",
    "    \n",
    "    # Example 2: Batch processing\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Batch Processing with Language Detection\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_files = [\n",
    "\n",
    "        r\"test_audio\\AI\\voice_preview_faiq - standard, clear and neutral.mp3\",\n",
    "        r\"test_audio\\AI\\clova.mp3\",\n",
    "        r\"test_audio\\AI\\sample voice 1.mp3\",\n",
    "        r\"test_audio\\AI\\voice_preview_mukundan - formal and clear.mp3\",\n",
    "        r\"test_audio\\AI\\voice_preview_kanika - soft, smooth and muffled.mp3\",\n",
    "        r\"test_audio\\AI\\medieval-gamer-voice-darkness-hunts-us-what-youx27ve-learned-stay-226596.mp3\",\n",
    "        r\"test_audio\\AI\\voice_preview_tarini - soft, cheerful and expressive.mp3\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Filename':<40} {'Language':<15} {'Classification':<15} {'Confidence'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            result = detector.analyze(file_path, input_type=\"file\")\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                filename = os.path.basename(file_path)[:37] + \"...\"\n",
    "                print(f\"{filename:<40} {result['language']:<15} {result['classification']:<15} {result['confidenceScore']:.2f}\")\n",
    "                print(f\"\\n🔍 Debug Info:\")\n",
    "                for key, val in result.get('debug', {}).items():\n",
    "                    print(f\"   {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221493a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
